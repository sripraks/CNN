{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainTest_ResNet_tinyImageNet_200_WithDecay.ipynb",
      "provenance": [],
      "mount_file_id": "1XWw8QpumUrGXeO4QL5_0DhTe_ErZUXEz",
      "authorship_tag": "ABX9TyMg0W/dz4L2RsUjfIGqoO45",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sripraks/CNN/blob/main/tinyImageNet-200/TrainTest_ResNet_tinyImageNet_200_WithDecay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTrvrjlyKdbR"
      },
      "source": [
        "import matplotlib\r\n",
        "matplotlib.use(\"Agg\")\r\n",
        "\r\n",
        "# import the necessary packages\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\r\n",
        "from tensorflow.keras.optimizers import SGD\r\n",
        "import argparse\r\n",
        "import json\r\n",
        "import sys\r\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM-lUfSsKrt0"
      },
      "source": [
        "# import the necessary packages\r\n",
        "from os import path\r\n",
        "\r\n",
        "# since we do not have access to the testing data we need to\r\n",
        "# take a number of images from the training data and use it instead\r\n",
        "NUM_CLASSES = 200\r\n",
        "NUM_TEST_IMAGES = 50 * NUM_CLASSES\r\n",
        "\r\n",
        "# define the path to the output training, validation, and testing\r\n",
        "# HDF5 files\r\n",
        "TRAIN_HDF5 = \"/content/drive/MyDrive/dataset/tiny-imagenet-200-hdf5/train.hdf5\"\r\n",
        "VAL_HDF5 = \"/content/drive/MyDrive/dataset/tiny-imagenet-200-hdf5/val.hdf5\"\r\n",
        "TEST_HDF5 = \"/content/drive/MyDrive/dataset/tiny-imagenet-200-hdf5/test.hdf5\"\r\n",
        "\r\n",
        "# define the path to the dataset mean\r\n",
        "DATASET_MEAN = \"/content/drive/MyDrive/dataset/tiny-imagenet-200-hdf5/output/tiny-image-net-200-mean.json\"\r\n",
        "\r\n",
        "# define the path to the output directory used for storing plots,\r\n",
        "# classification reports, etc.\r\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/dataset/tiny-imagenet-200-hdf5/output\"\r\n",
        "MODEL_PATH = path.sep.join([OUTPUT_PATH,\"resnet_tinyimagenet_withdecay.hdf5\"])\r\n",
        "FIG_PATH = path.sep.join([OUTPUT_PATH,\"resnet56_tinyimagenet_withdecay.png\"])\r\n",
        "JSON_PATH = path.sep.join([OUTPUT_PATH,\"resnet56_tinyimagenet_withdecay.json\"])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKpAZc1vKuoX"
      },
      "source": [
        "# import the necessary packages\r\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\r\n",
        "\r\n",
        "class ImageToArrayPreprocessor:\r\n",
        "\tdef __init__(self, dataFormat=None):\r\n",
        "\t\t# store the image data format\r\n",
        "\t\tself.dataFormat = dataFormat\r\n",
        "\r\n",
        "\tdef preprocess(self, image):\r\n",
        "\t\t# apply the Keras utility function that correctly rearranges\r\n",
        "\t\t# the dimensions of the image\r\n",
        "\t\treturn img_to_array(image, data_format=self.dataFormat)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQBIGGEQKwuV"
      },
      "source": [
        "# import the necessary packages\r\n",
        "import cv2\r\n",
        "\r\n",
        "class SimplePreprocessor:\r\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\r\n",
        "\t\t# store the target image width, height, and interpolation\r\n",
        "\t\t# method used when resizing\r\n",
        "\t\tself.width = width\r\n",
        "\t\tself.height = height\r\n",
        "\t\tself.inter = inter\r\n",
        "\r\n",
        "\tdef preprocess(self, image):\r\n",
        "\t\t# resize the image to a fixed size, ignoring the aspect\r\n",
        "\t\t# ratio\r\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\r\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAdBkLvxKyll"
      },
      "source": [
        "\r\n",
        "# import the necessary packages\r\n",
        "import cv2\r\n",
        "\r\n",
        "class MeanPreprocessor:\r\n",
        "\tdef __init__(self, rMean, gMean, bMean):\r\n",
        "\t\t# store the Red, Green, and Blue channel averages across a\r\n",
        "\t\t# training set\r\n",
        "\t\tself.rMean = rMean\r\n",
        "\t\tself.gMean = gMean\r\n",
        "\t\tself.bMean = bMean\r\n",
        "\r\n",
        "\tdef preprocess(self, image):\r\n",
        "\t\t# split the image into its respective Red, Green, and Blue\r\n",
        "\t\t# channels\r\n",
        "\t\t(B, G, R) = cv2.split(image.astype(\"float32\"))\r\n",
        "\r\n",
        "\t\t# subtract the means for each channel\r\n",
        "\t\tR -= self.rMean\r\n",
        "\t\tG -= self.gMean\r\n",
        "\t\tB -= self.bMean\r\n",
        "\r\n",
        "\t\t# merge the channels back together and return the image\r\n",
        "\t\treturn cv2.merge([B, G, R])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti0iF7awK5uk"
      },
      "source": [
        "# import the necessary packages\r\n",
        "from tensorflow.keras.callbacks import BaseLogger\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "import os\r\n",
        "\r\n",
        "class TrainingMonitor(BaseLogger):\r\n",
        "\tdef __init__(self, figPath, jsonPath=None, startAt=0):\r\n",
        "\t\t# store the output path for the figure, the path to the JSON\r\n",
        "\t\t# serialized file, and the starting epoch\r\n",
        "\t\tsuper(TrainingMonitor, self).__init__()\r\n",
        "\t\tself.figPath = figPath\r\n",
        "\t\tself.jsonPath = jsonPath\r\n",
        "\t\tself.startAt = startAt\r\n",
        "\r\n",
        "\tdef on_train_begin(self, logs={}):\r\n",
        "\t\t# initialize the history dictionary\r\n",
        "\t\tself.H = {}\r\n",
        "\r\n",
        "\t\t# if the JSON history path exists, load the training history\r\n",
        "\t\tif self.jsonPath is not None:\r\n",
        "\t\t\tif os.path.exists(self.jsonPath):\r\n",
        "\t\t\t\tself.H = json.loads(open(self.jsonPath).read())\r\n",
        "\r\n",
        "\t\t\t\t# check to see if a starting epoch was supplied\r\n",
        "\t\t\t\tif self.startAt > 0:\r\n",
        "\t\t\t\t\t# loop over the entries in the history log and\r\n",
        "\t\t\t\t\t# trim any entries that are past the starting\r\n",
        "\t\t\t\t\t# epoch\r\n",
        "\t\t\t\t\tfor k in self.H.keys():\r\n",
        "\t\t\t\t\t\tself.H[k] = self.H[k][:self.startAt]\r\n",
        "\r\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\r\n",
        "\t\t# loop over the logs and update the loss, accuracy, etc.\r\n",
        "\t\t# for the entire training process\r\n",
        "\t\tfor (k, v) in logs.items():\r\n",
        "\t\t\tl = self.H.get(k, [])\r\n",
        "\t\t\tl.append(float(v))\r\n",
        "\t\t\tself.H[k] = l\r\n",
        "\r\n",
        "\t\t# check to see if the training history should be serialized\r\n",
        "\t\t# to file\r\n",
        "\t\tif self.jsonPath is not None:\r\n",
        "\t\t\tf = open(self.jsonPath, \"w\")\r\n",
        "\t\t\tf.write(json.dumps(self.H))\r\n",
        "\t\t\tf.close()\r\n",
        "\r\n",
        "\t\t# ensure at least two epochs have passed before plotting\r\n",
        "\t\t# (epoch starts at zero)\r\n",
        "\t\tif len(self.H[\"loss\"]) > 1:\r\n",
        "\t\t\t# plot the training loss and accuracy\r\n",
        "\t\t\tN = np.arange(0, len(self.H[\"loss\"]))\r\n",
        "\t\t\tplt.style.use(\"ggplot\")\r\n",
        "\t\t\tplt.figure()\r\n",
        "\t\t\tplt.plot(N, self.H[\"loss\"], label=\"train_loss\")\r\n",
        "\t\t\tplt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\r\n",
        "\t\t\tplt.plot(N, self.H[\"accuracy\"], label=\"train_acc\")\r\n",
        "\t\t\tplt.plot(N, self.H[\"val_accuracy\"], label=\"val_acc\")\r\n",
        "\t\t\tplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(\r\n",
        "\t\t\t\tlen(self.H[\"loss\"])))\r\n",
        "\t\t\tplt.xlabel(\"Epoch #\")\r\n",
        "\t\t\tplt.ylabel(\"Loss/Accuracy\")\r\n",
        "\t\t\tplt.legend()\r\n",
        "\r\n",
        "\t\t\t# save the figure\r\n",
        "\t\t\tplt.savefig(self.figPath)\r\n",
        "\t\t\tplt.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbwtCPkIK8ur"
      },
      "source": [
        "# import the necessary packages\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "import numpy as np\r\n",
        "import h5py\r\n",
        "\r\n",
        "class HDF5DatasetGenerator:\r\n",
        "\tdef __init__(self, dbPath, batchSize, preprocessors=None,\r\n",
        "\t\taug=None, binarize=True, classes=2):\r\n",
        "\t\t# store the batch size, preprocessors, and data augmentor,\r\n",
        "\t\t# whether or not the labels should be binarized, along with\r\n",
        "\t\t# the total number of classes\r\n",
        "\t\tself.batchSize = batchSize\r\n",
        "\t\tself.preprocessors = preprocessors\r\n",
        "\t\tself.aug = aug\r\n",
        "\t\tself.binarize = binarize\r\n",
        "\t\tself.classes = classes\r\n",
        "\r\n",
        "\t\t# open the HDF5 database for reading and determine the total\r\n",
        "\t\t# number of entries in the database\r\n",
        "\t\tself.db = h5py.File(dbPath, \"r\")\r\n",
        "\t\tself.numImages = self.db[\"labels\"].shape[0]\r\n",
        "\r\n",
        "\tdef generator(self, passes=np.inf):\r\n",
        "\t\t# initialize the epoch count\r\n",
        "\t\tepochs = 0\r\n",
        "\r\n",
        "\t\t# keep looping infinitely -- the model will stop once we have\r\n",
        "\t\t# reach the desired number of epochs\r\n",
        "\t\twhile epochs < passes:\r\n",
        "\t\t\t# loop over the HDF5 dataset\r\n",
        "\t\t\tfor i in np.arange(0, self.numImages, self.batchSize):\r\n",
        "\t\t\t\t# extract the images and labels from the HDF dataset\r\n",
        "\t\t\t\timages = self.db[\"images\"][i: i + self.batchSize]\r\n",
        "\t\t\t\tlabels = self.db[\"labels\"][i: i + self.batchSize]\r\n",
        "\r\n",
        "\t\t\t\t# check to see if the labels should be binarized\r\n",
        "\t\t\t\tif self.binarize:\r\n",
        "\t\t\t\t\tlabels = to_categorical(labels,\r\n",
        "\t\t\t\t\t\tself.classes)\r\n",
        "\r\n",
        "\t\t\t\t# check to see if our preprocessors are not None\r\n",
        "\t\t\t\tif self.preprocessors is not None:\r\n",
        "\t\t\t\t\t# initialize the list of processed images\r\n",
        "\t\t\t\t\tprocImages = []\r\n",
        "\r\n",
        "\t\t\t\t\t# loop over the images\r\n",
        "\t\t\t\t\tfor image in images:\r\n",
        "\t\t\t\t\t\t# loop over the preprocessors and apply each\r\n",
        "\t\t\t\t\t\t# to the image\r\n",
        "\t\t\t\t\t\tfor p in self.preprocessors:\r\n",
        "\t\t\t\t\t\t\timage = p.preprocess(image)\r\n",
        "\r\n",
        "\t\t\t\t\t\t# update the list of processed images\r\n",
        "\t\t\t\t\t\tprocImages.append(image)\r\n",
        "\r\n",
        "\t\t\t\t\t# update the images array to be the processed\r\n",
        "\t\t\t\t\t# images\r\n",
        "\t\t\t\t\timages = np.array(procImages)\r\n",
        "\r\n",
        "\t\t\t\t# if the data augmenator exists, apply it\r\n",
        "\t\t\t\tif self.aug is not None:\r\n",
        "\t\t\t\t\t(images, labels) = next(self.aug.flow(images,\r\n",
        "\t\t\t\t\t\tlabels, batch_size=self.batchSize))\r\n",
        "\r\n",
        "\t\t\t\t# yield a tuple of images and labels\r\n",
        "\t\t\t\tyield (images, labels)\r\n",
        "\r\n",
        "\t\t\t# increment the total number of epochs\r\n",
        "\t\t\tepochs += 1\r\n",
        "\r\n",
        "\tdef close(self):\r\n",
        "\t\t# close the database\r\n",
        "\t\tself.db.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne64BnkpLD4R"
      },
      "source": [
        "# import the necessary packages\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.layers import Conv2D\r\n",
        "from tensorflow.keras.layers import AveragePooling2D\r\n",
        "from tensorflow.keras.layers import MaxPooling2D\r\n",
        "from tensorflow.keras.layers import ZeroPadding2D\r\n",
        "from tensorflow.keras.layers import Activation\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import Flatten\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import add\r\n",
        "from tensorflow.keras.regularizers import l2\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "\r\n",
        "class ResNet:\r\n",
        "\t@staticmethod\r\n",
        "\tdef residual_module(data, K, stride, chanDim, red=False,\r\n",
        "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\r\n",
        "\t\t# the shortcut branch of the ResNet module should be\r\n",
        "\t\t# initialize as the input (identity) data\r\n",
        "\t\tshortcut = data\r\n",
        "\r\n",
        "\t\t# the first block of the ResNet module are the 1x1 CONVs\r\n",
        "\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\tmomentum=bnMom)(data)\r\n",
        "\t\tact1 = Activation(\"relu\")(bn1)\r\n",
        "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\r\n",
        "\t\t\tkernel_regularizer=l2(reg))(act1)\r\n",
        "\r\n",
        "\t\t# the second block of the ResNet module are the 3x3 CONVs\r\n",
        "\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\tmomentum=bnMom)(conv1)\r\n",
        "\t\tact2 = Activation(\"relu\")(bn2)\r\n",
        "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\r\n",
        "\t\t\tpadding=\"same\", use_bias=False,\r\n",
        "\t\t\tkernel_regularizer=l2(reg))(act2)\r\n",
        "\r\n",
        "\t\t# the third block of the ResNet module is another set of 1x1\r\n",
        "\t\t# CONVs\r\n",
        "\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\tmomentum=bnMom)(conv2)\r\n",
        "\t\tact3 = Activation(\"relu\")(bn3)\r\n",
        "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\r\n",
        "\t\t\tkernel_regularizer=l2(reg))(act3)\r\n",
        "\r\n",
        "\t\t# if we are to reduce the spatial size, apply a CONV layer to\r\n",
        "\t\t# the shortcut\r\n",
        "\t\tif red:\r\n",
        "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\r\n",
        "\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\r\n",
        "\r\n",
        "\t\t# add together the shortcut and the final CONV\r\n",
        "\t\tx = add([conv3, shortcut])\r\n",
        "\r\n",
        "\t\t# return the addition as the output of the ResNet module\r\n",
        "\t\treturn x\r\n",
        "\r\n",
        "\t@staticmethod\r\n",
        "\tdef build(width, height, depth, classes, stages, filters,\r\n",
        "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\r\n",
        "\t\t# initialize the input shape to be \"channels last\" and the\r\n",
        "\t\t# channels dimension itself\r\n",
        "\t\tinputShape = (height, width, depth)\r\n",
        "\t\tchanDim = -1\r\n",
        "\r\n",
        "\t\t# if we are using \"channels first\", update the input shape\r\n",
        "\t\t# and channels dimension\r\n",
        "\t\tif K.image_data_format() == \"channels_first\":\r\n",
        "\t\t\tinputShape = (depth, height, width)\r\n",
        "\t\t\tchanDim = 1\r\n",
        "\r\n",
        "\t\t# set the input and apply BN\r\n",
        "\t\tinputs = Input(shape=inputShape)\r\n",
        "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\tmomentum=bnMom)(inputs)\r\n",
        "\r\n",
        "\t\t# check if we are utilizing the CIFAR dataset\r\n",
        "\t\tif dataset == \"cifar\":\r\n",
        "\t\t\t# apply a single CONV layer\r\n",
        "\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\r\n",
        "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\r\n",
        "\r\n",
        "\t\t# check to see if we are using the Tiny ImageNet dataset\r\n",
        "\t\telif dataset == \"tiny_imagenet\":\r\n",
        "\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\r\n",
        "\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\r\n",
        "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\r\n",
        "\t\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\t\tmomentum=bnMom)(x)\r\n",
        "\t\t\tx = Activation(\"relu\")(x)\r\n",
        "\t\t\tx = ZeroPadding2D((1, 1))(x)\r\n",
        "\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\r\n",
        "\r\n",
        "\t\t# loop over the number of stages\r\n",
        "\t\tfor i in range(0, len(stages)):\r\n",
        "\t\t\t# initialize the stride, then apply a residual module\r\n",
        "\t\t\t# used to reduce the spatial size of the input volume\r\n",
        "\t\t\tstride = (1, 1) if i == 0 else (2, 2)\r\n",
        "\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\r\n",
        "\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\r\n",
        "\r\n",
        "\t\t\t# loop over the number of layers in the stage\r\n",
        "\t\t\tfor j in range(0, stages[i] - 1):\r\n",
        "\t\t\t\t# apply a ResNet module\r\n",
        "\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\r\n",
        "\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\r\n",
        "\r\n",
        "\t\t# apply BN => ACT => POOL\r\n",
        "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\tmomentum=bnMom)(x)\r\n",
        "\t\tx = Activation(\"relu\")(x)\r\n",
        "\t\tx = AveragePooling2D((8, 8))(x)\r\n",
        "\r\n",
        "\t\t# softmax classifier\r\n",
        "\t\tx = Flatten()(x)\r\n",
        "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\r\n",
        "\t\tx = Activation(\"softmax\")(x)\r\n",
        "\r\n",
        "\t\t# create the model\r\n",
        "\t\tmodel = Model(inputs, x, name=\"resnet\")\r\n",
        "\r\n",
        "\t\t# return the constructed network architecture\r\n",
        "\t\treturn model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q5dHl4CLJDZ"
      },
      "source": [
        "# set a high recursion limit so Theano doesn't complain\r\n",
        "sys.setrecursionlimit(5000)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIfqR0h3LKBA"
      },
      "source": [
        "# define the total number of epochs to train for along with the\r\n",
        "# initial learning rate\r\n",
        "NUM_EPOCHS = 75\r\n",
        "INIT_LR = 1e-1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOj_wyuaLMdz"
      },
      "source": [
        "def poly_decay(epoch):\r\n",
        "\t# initialize the maximum number of epochs, base learning rate,\r\n",
        "\t# and power of the polynomial\r\n",
        "\tmaxEpochs = NUM_EPOCHS\r\n",
        "\tbaseLR = INIT_LR\r\n",
        "\tpower = 1.0\r\n",
        "\r\n",
        "\t# compute the new learning rate based on polynomial decay\r\n",
        "\talpha = baseLR * (1 - (epoch / float(maxEpochs))) ** power\r\n",
        "\r\n",
        "\t# return the new learning rate\r\n",
        "\treturn alpha\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_CrNE12LOoR"
      },
      "source": [
        "# construct the training image generator for data augmentation\r\n",
        "aug = ImageDataGenerator(rotation_range=18, zoom_range=0.15,\r\n",
        "\twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\r\n",
        "\thorizontal_flip=True, fill_mode=\"nearest\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2FLVYpyLQET"
      },
      "source": [
        "# load the RGB means for the training set\r\n",
        "means = json.loads(open(DATASET_MEAN).read())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z96uetpKLRWB"
      },
      "source": [
        "# initialize the image preprocessors\r\n",
        "sp = SimplePreprocessor(64, 64)\r\n",
        "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\r\n",
        "iap = ImageToArrayPreprocessor()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzuAdUhQLS2B"
      },
      "source": [
        "# initialize the training and validation dataset generators\r\n",
        "trainGen = HDF5DatasetGenerator(TRAIN_HDF5, 64, aug=aug,\r\n",
        "\tpreprocessors=[sp, mp, iap], classes=NUM_CLASSES)\r\n",
        "valGen = HDF5DatasetGenerator(VAL_HDF5, 64,\r\n",
        "\tpreprocessors=[sp, mp, iap], classes=NUM_CLASSES)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDTs1aooLUNB"
      },
      "source": [
        "# construct the set of callbacks\r\n",
        "figPath = os.path.sep.join([\"/content/drive/MyDrive/dataset/tiny-imagenet-200-hdf5/output\", \"{}.png\".format(os.getpid())])\r\n",
        "jsonPath = os.path.sep.join([\"/content/drive/MyDrive/dataset/tiny-imagenet-200-hdf5/output\", \"{}.json\".format(os.getpid())])\r\n",
        "callbacks = [TrainingMonitor(figPath, jsonPath=jsonPath),\r\n",
        "\tLearningRateScheduler(poly_decay)]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot7tGDSXLVlR",
        "outputId": "a708faa0-b02d-4404-d61e-831c5f36fe4d"
      },
      "source": [
        "# initialize the optimizer and model (ResNet-56)\r\n",
        "print(\"[INFO] compiling model...\")\r\n",
        "model = ResNet.build(64, 64, 3, NUM_CLASSES, (3, 4, 6),(64, 128, 256, 512), reg=0.0005, dataset=\"tiny_imagenet\")\r\n",
        "opt = SGD(lr=INIT_LR, momentum=0.9)\r\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V4vrDftLXGk",
        "outputId": "5ce7fa4e-23c2-458e-cd93-133aac043b58"
      },
      "source": [
        "# train the network\r\n",
        "print(\"[INFO] training network...\")\r\n",
        "model.fit_generator(\r\n",
        "\ttrainGen.generator(),\r\n",
        "\tsteps_per_epoch=trainGen.numImages // 64,\r\n",
        "\tvalidation_data=valGen.generator(),\r\n",
        "\tvalidation_steps=valGen.numImages // 64,\r\n",
        "\tepochs=NUM_EPOCHS,\r\n",
        "\tmax_queue_size=10,\r\n",
        "\tcallbacks=callbacks, verbose=1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/75\n",
            "1406/1406 [==============================] - 229s 155ms/step - loss: 5.5811 - accuracy: 0.0356 - val_loss: 4.9587 - val_accuracy: 0.0808\n",
            "Epoch 2/75\n",
            "1406/1406 [==============================] - 275s 195ms/step - loss: 4.6713 - accuracy: 0.1108 - val_loss: 4.4489 - val_accuracy: 0.1452\n",
            "Epoch 3/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 4.3419 - accuracy: 0.1579 - val_loss: 4.3206 - val_accuracy: 0.1642\n",
            "Epoch 4/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 4.1505 - accuracy: 0.1918 - val_loss: 4.3983 - val_accuracy: 0.1691\n",
            "Epoch 5/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 4.0188 - accuracy: 0.2194 - val_loss: 4.1577 - val_accuracy: 0.2090\n",
            "Epoch 6/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.9342 - accuracy: 0.2419 - val_loss: 4.3258 - val_accuracy: 0.1986\n",
            "Epoch 7/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.8688 - accuracy: 0.2610 - val_loss: 4.1911 - val_accuracy: 0.2271\n",
            "Epoch 8/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.8209 - accuracy: 0.2743 - val_loss: 4.6444 - val_accuracy: 0.1923\n",
            "Epoch 9/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.7905 - accuracy: 0.2869 - val_loss: 4.8495 - val_accuracy: 0.1878\n",
            "Epoch 10/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.7526 - accuracy: 0.2962 - val_loss: 4.3376 - val_accuracy: 0.2296\n",
            "Epoch 11/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.7211 - accuracy: 0.3079 - val_loss: 4.6345 - val_accuracy: 0.2095\n",
            "Epoch 12/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.6895 - accuracy: 0.3202 - val_loss: 4.1346 - val_accuracy: 0.2676\n",
            "Epoch 13/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.6595 - accuracy: 0.3319 - val_loss: 4.9830 - val_accuracy: 0.2000\n",
            "Epoch 14/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.6400 - accuracy: 0.3396 - val_loss: 4.5474 - val_accuracy: 0.2406\n",
            "Epoch 15/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.6188 - accuracy: 0.3478 - val_loss: 4.1312 - val_accuracy: 0.2796\n",
            "Epoch 16/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.5949 - accuracy: 0.3536 - val_loss: 4.3368 - val_accuracy: 0.2687\n",
            "Epoch 17/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.5675 - accuracy: 0.3638 - val_loss: 4.7394 - val_accuracy: 0.2499\n",
            "Epoch 18/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.5520 - accuracy: 0.3693 - val_loss: 4.2316 - val_accuracy: 0.2932\n",
            "Epoch 19/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.5341 - accuracy: 0.3757 - val_loss: 4.6189 - val_accuracy: 0.2537\n",
            "Epoch 20/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.5175 - accuracy: 0.3827 - val_loss: 4.1134 - val_accuracy: 0.3033\n",
            "Epoch 21/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.4960 - accuracy: 0.3873 - val_loss: 4.0775 - val_accuracy: 0.3121\n",
            "Epoch 22/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.4753 - accuracy: 0.3937 - val_loss: 4.1569 - val_accuracy: 0.3190\n",
            "Epoch 23/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.4606 - accuracy: 0.3980 - val_loss: 4.1931 - val_accuracy: 0.3066\n",
            "Epoch 24/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.4366 - accuracy: 0.4043 - val_loss: 4.3890 - val_accuracy: 0.2894\n",
            "Epoch 25/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.4237 - accuracy: 0.4074 - val_loss: 4.3129 - val_accuracy: 0.3009\n",
            "Epoch 26/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.4095 - accuracy: 0.4114 - val_loss: 4.1062 - val_accuracy: 0.3298\n",
            "Epoch 27/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.3868 - accuracy: 0.4178 - val_loss: 4.2356 - val_accuracy: 0.3211\n",
            "Epoch 28/75\n",
            "1406/1406 [==============================] - 275s 195ms/step - loss: 3.3683 - accuracy: 0.4225 - val_loss: 4.2632 - val_accuracy: 0.3122\n",
            "Epoch 29/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.3449 - accuracy: 0.4284 - val_loss: 4.2055 - val_accuracy: 0.3222\n",
            "Epoch 30/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.3284 - accuracy: 0.4335 - val_loss: 4.1088 - val_accuracy: 0.3404\n",
            "Epoch 31/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.3093 - accuracy: 0.4393 - val_loss: 4.0164 - val_accuracy: 0.3520\n",
            "Epoch 32/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.2829 - accuracy: 0.4434 - val_loss: 3.9655 - val_accuracy: 0.3580\n",
            "Epoch 33/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.2680 - accuracy: 0.4485 - val_loss: 4.0367 - val_accuracy: 0.3448\n",
            "Epoch 34/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.2511 - accuracy: 0.4542 - val_loss: 4.1863 - val_accuracy: 0.3189\n",
            "Epoch 35/75\n",
            "1406/1406 [==============================] - 275s 195ms/step - loss: 3.2316 - accuracy: 0.4556 - val_loss: 3.8971 - val_accuracy: 0.3696\n",
            "Epoch 36/75\n",
            "1406/1406 [==============================] - 275s 195ms/step - loss: 3.2024 - accuracy: 0.4631 - val_loss: 3.9467 - val_accuracy: 0.3686\n",
            "Epoch 37/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.1839 - accuracy: 0.4683 - val_loss: 3.9449 - val_accuracy: 0.3638\n",
            "Epoch 38/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.1581 - accuracy: 0.4738 - val_loss: 3.9571 - val_accuracy: 0.3657\n",
            "Epoch 39/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.1528 - accuracy: 0.4710 - val_loss: 3.8682 - val_accuracy: 0.3690\n",
            "Epoch 40/75\n",
            "1406/1406 [==============================] - 275s 195ms/step - loss: 3.1222 - accuracy: 0.4829 - val_loss: 3.8205 - val_accuracy: 0.3909\n",
            "Epoch 41/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.0990 - accuracy: 0.4845 - val_loss: 3.8687 - val_accuracy: 0.3801\n",
            "Epoch 42/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.0742 - accuracy: 0.4907 - val_loss: 3.8325 - val_accuracy: 0.3967\n",
            "Epoch 43/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.0547 - accuracy: 0.4951 - val_loss: 3.9796 - val_accuracy: 0.3708\n",
            "Epoch 44/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 3.0190 - accuracy: 0.5013 - val_loss: 3.9997 - val_accuracy: 0.3649\n",
            "Epoch 45/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.9981 - accuracy: 0.5056 - val_loss: 3.6688 - val_accuracy: 0.4174\n",
            "Epoch 46/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.9818 - accuracy: 0.5084 - val_loss: 3.8009 - val_accuracy: 0.4030\n",
            "Epoch 47/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.9445 - accuracy: 0.5159 - val_loss: 3.8074 - val_accuracy: 0.3974\n",
            "Epoch 48/75\n",
            "1406/1406 [==============================] - 275s 195ms/step - loss: 2.9205 - accuracy: 0.5191 - val_loss: 4.1496 - val_accuracy: 0.3816\n",
            "Epoch 49/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.8988 - accuracy: 0.5262 - val_loss: 3.8436 - val_accuracy: 0.4023\n",
            "Epoch 50/75\n",
            "1406/1406 [==============================] - 273s 194ms/step - loss: 2.8614 - accuracy: 0.5302 - val_loss: 4.0861 - val_accuracy: 0.3846\n",
            "Epoch 51/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.8218 - accuracy: 0.5402 - val_loss: 4.0057 - val_accuracy: 0.3861\n",
            "Epoch 52/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.8063 - accuracy: 0.5405 - val_loss: 3.9494 - val_accuracy: 0.3864\n",
            "Epoch 53/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.7567 - accuracy: 0.5505 - val_loss: 4.0149 - val_accuracy: 0.3941\n",
            "Epoch 54/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.7308 - accuracy: 0.5575 - val_loss: 3.9319 - val_accuracy: 0.3826\n",
            "Epoch 55/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.6940 - accuracy: 0.5644 - val_loss: 4.1180 - val_accuracy: 0.3698\n",
            "Epoch 56/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.6608 - accuracy: 0.5718 - val_loss: 3.6425 - val_accuracy: 0.4364\n",
            "Epoch 57/75\n",
            "1406/1406 [==============================] - 275s 195ms/step - loss: 2.6122 - accuracy: 0.5786 - val_loss: 4.0760 - val_accuracy: 0.3884\n",
            "Epoch 58/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.5717 - accuracy: 0.5850 - val_loss: 3.9025 - val_accuracy: 0.4027\n",
            "Epoch 59/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.5352 - accuracy: 0.5911 - val_loss: 4.1328 - val_accuracy: 0.3854\n",
            "Epoch 60/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.4872 - accuracy: 0.6017 - val_loss: 3.9591 - val_accuracy: 0.4082\n",
            "Epoch 61/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.4367 - accuracy: 0.6100 - val_loss: 3.6700 - val_accuracy: 0.4401\n",
            "Epoch 62/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.3862 - accuracy: 0.6212 - val_loss: 3.8733 - val_accuracy: 0.4147\n",
            "Epoch 63/75\n",
            "1406/1406 [==============================] - 275s 195ms/step - loss: 2.3426 - accuracy: 0.6288 - val_loss: 3.7036 - val_accuracy: 0.4323\n",
            "Epoch 64/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.2804 - accuracy: 0.6398 - val_loss: 3.5783 - val_accuracy: 0.4350\n",
            "Epoch 65/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.2315 - accuracy: 0.6492 - val_loss: 3.7324 - val_accuracy: 0.4336\n",
            "Epoch 66/75\n",
            "1406/1406 [==============================] - 275s 195ms/step - loss: 2.1623 - accuracy: 0.6621 - val_loss: 3.6268 - val_accuracy: 0.4480\n",
            "Epoch 67/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.0923 - accuracy: 0.6751 - val_loss: 3.4630 - val_accuracy: 0.4647\n",
            "Epoch 68/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 2.0272 - accuracy: 0.6902 - val_loss: 3.4229 - val_accuracy: 0.4720\n",
            "Epoch 69/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 1.9384 - accuracy: 0.7064 - val_loss: 3.3466 - val_accuracy: 0.4917\n",
            "Epoch 70/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 1.8791 - accuracy: 0.7203 - val_loss: 3.6693 - val_accuracy: 0.4534\n",
            "Epoch 71/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 1.7894 - accuracy: 0.7407 - val_loss: 3.5831 - val_accuracy: 0.4619\n",
            "Epoch 72/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 1.7087 - accuracy: 0.7585 - val_loss: 3.4735 - val_accuracy: 0.4811\n",
            "Epoch 73/75\n",
            "1406/1406 [==============================] - 275s 196ms/step - loss: 1.6319 - accuracy: 0.7753 - val_loss: 3.4641 - val_accuracy: 0.4822\n",
            "Epoch 74/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 1.5548 - accuracy: 0.7923 - val_loss: 3.4584 - val_accuracy: 0.4829\n",
            "Epoch 75/75\n",
            "1406/1406 [==============================] - 274s 195ms/step - loss: 1.4891 - accuracy: 0.8087 - val_loss: 3.4610 - val_accuracy: 0.4905\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fec9bfdac18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6AwJUofLYcX",
        "outputId": "641325a4-5c9b-4f3b-f61a-8ef879ba3b03"
      },
      "source": [
        "# save the network to disk\r\n",
        "print(\"[INFO] serializing network...\")\r\n",
        "model.save(\"/content/drive/MyDrive/dataset/tiny-imagenet-200-hdf5/output/resnet_tinyimagenet_decay.hdf5\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] serializing network...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrg3yzSHLZiU"
      },
      "source": [
        "# close the databases\r\n",
        "trainGen.close()\r\n",
        "valGen.close()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq0L0czfM1Zm"
      },
      "source": [
        "# import the necessary packages\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def rank5_accuracy(preds, labels):\r\n",
        "\t# initialize the rank-1 and rank-5 accuracies\r\n",
        "\trank1 = 0\r\n",
        "\trank5 = 0\r\n",
        "\r\n",
        "\t# loop over the predictions and ground-truth labels\r\n",
        "\tfor (p, gt) in zip(preds, labels):\r\n",
        "\t\t# sort the probabilities by their index in descending\r\n",
        "\t\t# order so that the more confident guesses are at the\r\n",
        "\t\t# front of the list\r\n",
        "\t\tp = np.argsort(p)[::-1]\r\n",
        "\r\n",
        "\t\t# check if the ground-truth label is in the top-5\r\n",
        "\t\t# predictions\r\n",
        "\t\tif gt in p[:5]:\r\n",
        "\t\t\trank5 += 1\r\n",
        "\r\n",
        "\t\t# check to see if the ground-truth is the #1 prediction\r\n",
        "\t\tif gt == p[0]:\r\n",
        "\t\t\trank1 += 1\r\n",
        "\r\n",
        "\t# compute the final rank-1 and rank-5 accuracies\r\n",
        "\trank1 /= float(len(preds))\r\n",
        "\trank5 /= float(len(preds))\r\n",
        "\r\n",
        "\t# return a tuple of the rank-1 and rank-5 accuracies\r\n",
        "\treturn (rank1, rank5)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJn-DQ6mM3_h"
      },
      "source": [
        "# load the RGB means for the training set\r\n",
        "means = json.loads(open(DATASET_MEAN).read())"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O17Ptdm4M56y"
      },
      "source": [
        "# initialize the image preprocessors\r\n",
        "sp = SimplePreprocessor(64, 64)\r\n",
        "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\r\n",
        "iap = ImageToArrayPreprocessor()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz1b3kQ0M8oV"
      },
      "source": [
        "# initialize the testing dataset generator\r\n",
        "testGen = HDF5DatasetGenerator(\"/content/drive/MyDrive/dataset/tiny-imagenet-200-hdf5/test.hdf5\", 64,preprocessors=[sp, mp, iap], classes=NUM_CLASSES)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSzoIhc0bNLa"
      },
      "source": [
        "from tensorflow.keras.models import load_model\r\n",
        "import json"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnescPVkM_IA",
        "outputId": "c6c90d7e-a38d-4537-9b91-e402907a3201"
      },
      "source": [
        "# load the pre-trained network\r\n",
        "print(\"[INFO] loading model...\")\r\n",
        "model = load_model(\"/content/drive/MyDrive/dataset/tiny-imagenet-200-hdf5/output/resnet_tinyimagenet_decay.hdf5\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LKub79NNBNv",
        "outputId": "2637dde2-e9ad-42b8-ef0c-92713f563fbc"
      },
      "source": [
        "# make predictions on the testing data\r\n",
        "print(\"[INFO] predicting on test data...\")\r\n",
        "predictions = model.predict_generator(testGen.generator(),\tsteps=testGen.numImages // 64, max_queue_size=10)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] predicting on test data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpAPWu5SNDp_",
        "outputId": "18ef0961-1d3d-46a6-f655-4a4a91a1a1f9"
      },
      "source": [
        "# compute the rank-1 and rank-5 accuracies\r\n",
        "(rank1, rank5) = rank5_accuracy(predictions, testGen.db[\"labels\"])\r\n",
        "print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\r\n",
        "print(\"[INFO] rank-5: {:.2f}%\".format(rank5 * 100))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] rank-1: 49.23%\n",
            "[INFO] rank-5: 73.77%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHH9J-sENGxz"
      },
      "source": [
        "testGen.close()"
      ],
      "execution_count": 33,
      "outputs": []
    }
  ]
}