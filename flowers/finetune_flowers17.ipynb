{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "finetune_flowers17.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sripraks/CNN/blob/main/flowers/finetune_flowers17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwMqGln0vUjN"
      },
      "source": [
        "### **Train VGG 16 on Flower Data set and Fine Tune With Augmentation + Top Layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U46D6hf8SmlJ"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "class ImageToArrayPreprocessor:\n",
        "\tdef __init__(self, dataFormat=None):\n",
        "\t\t# store the image data format\n",
        "\t\tself.dataFormat = dataFormat\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# apply the Keras utility function that correctly rearranges\n",
        "\t\t# the dimensions of the image\n",
        "\t\treturn img_to_array(image, data_format=self.dataFormat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ude659qSrmv"
      },
      "source": [
        "# import the necessary packages\n",
        "import imutils\n",
        "import cv2\n",
        "\n",
        "class AspectAwarePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# grab the dimensions of the image and then initialize\n",
        "\t\t# the deltas to use when cropping\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\tdW = 0\n",
        "\t\tdH = 0\n",
        "\n",
        "\t\t# if the width is smaller than the height, then resize\n",
        "\t\t# along the width (i.e., the smaller dimension) and then\n",
        "\t\t# update the deltas to crop the height to the desired\n",
        "\t\t# dimension\n",
        "\t\tif w < h:\n",
        "\t\t\timage = imutils.resize(image, width=self.width,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n",
        "\n",
        "\t\t# otherwise, the height is smaller than the width so\n",
        "\t\t# resize along the height and then update the deltas\n",
        "\t\t# crop along the width\n",
        "\t\telse:\n",
        "\t\t\timage = imutils.resize(image, height=self.height,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n",
        "\n",
        "\t\t# now that our images have been resized, we need to\n",
        "\t\t# re-grab the width and height, followed by performing\n",
        "\t\t# the crop\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\timage = image[dH:h - dH, dW:w - dW]\n",
        "\n",
        "\t\t# finally, resize the image to the provided spatial\n",
        "\t\t# dimensions to ensure our output image is always a fixed\n",
        "\t\t# size\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLnEbwnpS0FH"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "class SimpleDatasetLoader:\n",
        "\tdef __init__(self, preprocessors=None):\n",
        "\t\t# store the image preprocessor\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\n",
        "\t\t# if the preprocessors are None, initialize them as an\n",
        "\t\t# empty list\n",
        "\t\tif self.preprocessors is None:\n",
        "\t\t\tself.preprocessors = []\n",
        "\n",
        "\tdef load(self, imagePaths, verbose=-1):\n",
        "\t\t# initialize the list of features and labels\n",
        "\t\tdata = []\n",
        "\t\tlabels = []\n",
        "\n",
        "\t\t# loop over the input images\n",
        "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
        "\t\t\t# load the image and extract the class label assuming\n",
        "\t\t\t# that our path has the following format:\n",
        "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
        "\t\t\timage = cv2.imread(imagePath)\n",
        "\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t# loop over the preprocessors and apply each to\n",
        "\t\t\t\t# the image\n",
        "\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t# treat our processed image as a \"feature vector\"\n",
        "\t\t\t# by updating the data list followed by the labels\n",
        "\t\t\tdata.append(image)\n",
        "\t\t\tlabels.append(label)\n",
        "\n",
        "\t\t\t# show an update every `verbose` images\n",
        "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
        "\t\t\t\t\tlen(imagePaths)))\n",
        "\n",
        "\t\t# return a tuple of the data and labels\n",
        "\t\treturn (np.array(data), np.array(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jFMuhH9TADW"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class FCHeadNet:\n",
        "\t@staticmethod\n",
        "\tdef build(baseModel, classes, D):\n",
        "\t\t# initialize the head model that will be placed on top of\n",
        "\t\t# the base, then add a FC layer\n",
        "\t\theadModel = baseModel.output\n",
        "\t\theadModel = Flatten(name=\"flatten\")(headModel)\n",
        "\t\theadModel = Dense(D, activation=\"relu\")(headModel)\n",
        "\t\theadModel = Dropout(0.5)(headModel)\n",
        "\n",
        "\t\t# add a softmax layer\n",
        "\t\theadModel = Dense(classes, activation=\"softmax\")(headModel)\n",
        "\n",
        "\t\t# return the model\n",
        "\t\treturn headModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn_5W87YSFzf"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c20AqQRvSFzk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f27585-2814-4648-b504-3ec07017970e"
      },
      "source": [
        "# load the VGG16 network\n",
        "print(\"[INFO] loading network...\")\n",
        "model = VGG16(weights=\"imagenet\",include_top=True)\n",
        "print(\"[INFO] showing layers...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading network...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 3s 0us/step\n",
            "[INFO] showing layers...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSoWAxafSFzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d6dfbc-8293-4821-bfbe-1650b791a21f"
      },
      "source": [
        "# loop over the layers in the network and display them to the\n",
        "# console\n",
        "for (i, layer) in enumerate(model.layers):\n",
        "\tprint(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 0\tInputLayer\n",
            "[INFO] 1\tConv2D\n",
            "[INFO] 2\tConv2D\n",
            "[INFO] 3\tMaxPooling2D\n",
            "[INFO] 4\tConv2D\n",
            "[INFO] 5\tConv2D\n",
            "[INFO] 6\tMaxPooling2D\n",
            "[INFO] 7\tConv2D\n",
            "[INFO] 8\tConv2D\n",
            "[INFO] 9\tConv2D\n",
            "[INFO] 10\tMaxPooling2D\n",
            "[INFO] 11\tConv2D\n",
            "[INFO] 12\tConv2D\n",
            "[INFO] 13\tConv2D\n",
            "[INFO] 14\tMaxPooling2D\n",
            "[INFO] 15\tConv2D\n",
            "[INFO] 16\tConv2D\n",
            "[INFO] 17\tConv2D\n",
            "[INFO] 18\tMaxPooling2D\n",
            "[INFO] 19\tFlatten\n",
            "[INFO] 20\tDense\n",
            "[INFO] 21\tDense\n",
            "[INFO] 22\tDense\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6jnydoeSFzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c13f40-ddbb-474d-bee8-b9c79194c7a3"
      },
      "source": [
        "# load the VGG16 network\n",
        "print(\"[INFO] loading network...\")\n",
        "model = VGG16(weights=\"imagenet\",include_top=False)\n",
        "print(\"[INFO] showing layers...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading network...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "[INFO] showing layers...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYS4eE1eSFzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92ab6fa-1c4d-483d-a536-69d779cea154"
      },
      "source": [
        "# loop over the layers in the network and display them to the\n",
        "# console\n",
        "for (i, layer) in enumerate(model.layers):\n",
        "\tprint(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 0\tInputLayer\n",
            "[INFO] 1\tConv2D\n",
            "[INFO] 2\tConv2D\n",
            "[INFO] 3\tMaxPooling2D\n",
            "[INFO] 4\tConv2D\n",
            "[INFO] 5\tConv2D\n",
            "[INFO] 6\tMaxPooling2D\n",
            "[INFO] 7\tConv2D\n",
            "[INFO] 8\tConv2D\n",
            "[INFO] 9\tConv2D\n",
            "[INFO] 10\tMaxPooling2D\n",
            "[INFO] 11\tConv2D\n",
            "[INFO] 12\tConv2D\n",
            "[INFO] 13\tConv2D\n",
            "[INFO] 14\tMaxPooling2D\n",
            "[INFO] 15\tConv2D\n",
            "[INFO] 16\tConv2D\n",
            "[INFO] 17\tConv2D\n",
            "[INFO] 18\tMaxPooling2D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I16KixhlSFzu"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXBV_43QTKcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e1c16e2-fe83-4e1b-ec89-ea19462747a6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aQw_t3cSFzw"
      },
      "source": [
        "# construct the image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
        "\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "\thorizontal_flip=True, fill_mode=\"nearest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoxEaJTNSFzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b082d4-755c-4e3c-ff83-57b89a0f7ed8"
      },
      "source": [
        "# grab the list of images that we'll be describing, then extract\n",
        "# the class label names from the image paths\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(\"/content/drive/My Drive/dataset/flowers17/images\"))\n",
        "classNames = [pt.split(os.path.sep)[-2] for pt in imagePaths]\n",
        "classNames = [str(x) for x in np.unique(classNames)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7U3TxYwSFz0"
      },
      "source": [
        "# initialize the image preprocessors\n",
        "aap = AspectAwarePreprocessor(224, 224)\n",
        "iap = ImageToArrayPreprocessor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vICCgEZtSFz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea3a5b4-5ff2-4b45-be5b-47ab95870597"
      },
      "source": [
        "# load the dataset from disk then scale the raw pixel intensities to\n",
        "# the range [0, 1]\n",
        "sdl = SimpleDatasetLoader(preprocessors=[aap, iap])\n",
        "(data, labels) = sdl.load(imagePaths, verbose=500)\n",
        "data = data.astype(\"float\") / 255.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] processed 500/1360\n",
            "[INFO] processed 1000/1360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DDViRrxSFz3"
      },
      "source": [
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxPJNnFFSFz5"
      },
      "source": [
        "# convert the labels from integers to vectors\n",
        "trainY = LabelBinarizer().fit_transform(trainY)\n",
        "testY = LabelBinarizer().fit_transform(testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dka8CAfVSFz7"
      },
      "source": [
        "# load the VGG16 network, ensuring the head FC layer sets are left\n",
        "# off\n",
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,input_tensor=Input(shape=(224, 224, 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2cs8VM_SFz8"
      },
      "source": [
        "# initialize the new head of the network, a set of FC layers\n",
        "# followed by a softmax classifier\n",
        "headModel = FCHeadNet.build(baseModel, len(classNames), 256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOynKi0kSFz-"
      },
      "source": [
        "# place the head FC model on top of the base model -- this will\n",
        "# become the actual model we will train\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB0kRG1aSF0A"
      },
      "source": [
        "# loop over all layers in the base model and freeze them so they\n",
        "# will *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZD6NjcTSF0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b570c6dd-6416-482e-cfe8-ccc4078f1311"
      },
      "source": [
        "# compile our model (this needs to be done after our setting our\n",
        "# layers to being non-trainable\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = RMSprop(lr=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKZM7K9XSF0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c7c276-c2d9-4450-a06c-7001183cc41a"
      },
      "source": [
        "# train the head of the network for a few epochs (all other\n",
        "# layers are frozen) -- this will allow the new FC layers to\n",
        "# start to become initialized with actual \"learned\" values\n",
        "# versus pure random\n",
        "print(\"[INFO] training head...\")\n",
        "model.fit_generator(aug.flow(trainX, trainY, batch_size=32),\n",
        "\tvalidation_data=(testX, testY), epochs=25,\n",
        "\tsteps_per_epoch=len(trainX) // 32, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training head...\n",
            "WARNING:tensorflow:From <ipython-input-24-beee15ac1c5d>:8: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/25\n",
            "31/31 [==============================] - 14s 459ms/step - loss: 4.7478 - accuracy: 0.1609 - val_loss: 2.6238 - val_accuracy: 0.1912\n",
            "Epoch 2/25\n",
            "31/31 [==============================] - 12s 388ms/step - loss: 2.1445 - accuracy: 0.3259 - val_loss: 1.7469 - val_accuracy: 0.4324\n",
            "Epoch 3/25\n",
            "31/31 [==============================] - 12s 391ms/step - loss: 1.9032 - accuracy: 0.3877 - val_loss: 1.4760 - val_accuracy: 0.5441\n",
            "Epoch 4/25\n",
            "31/31 [==============================] - 12s 387ms/step - loss: 1.6475 - accuracy: 0.4696 - val_loss: 1.0756 - val_accuracy: 0.6971\n",
            "Epoch 5/25\n",
            "31/31 [==============================] - 12s 388ms/step - loss: 1.3954 - accuracy: 0.5597 - val_loss: 0.9785 - val_accuracy: 0.7118\n",
            "Epoch 6/25\n",
            "31/31 [==============================] - 12s 390ms/step - loss: 1.4319 - accuracy: 0.5425 - val_loss: 0.9635 - val_accuracy: 0.6853\n",
            "Epoch 7/25\n",
            "31/31 [==============================] - 12s 386ms/step - loss: 1.2453 - accuracy: 0.6144 - val_loss: 0.8242 - val_accuracy: 0.7353\n",
            "Epoch 8/25\n",
            "31/31 [==============================] - 12s 389ms/step - loss: 1.1196 - accuracy: 0.6366 - val_loss: 0.7900 - val_accuracy: 0.7618\n",
            "Epoch 9/25\n",
            "31/31 [==============================] - 12s 393ms/step - loss: 1.0365 - accuracy: 0.6670 - val_loss: 0.7485 - val_accuracy: 0.7529\n",
            "Epoch 10/25\n",
            "31/31 [==============================] - 12s 388ms/step - loss: 1.0249 - accuracy: 0.6822 - val_loss: 0.7792 - val_accuracy: 0.7353\n",
            "Epoch 11/25\n",
            "31/31 [==============================] - 12s 391ms/step - loss: 0.9908 - accuracy: 0.6842 - val_loss: 0.7228 - val_accuracy: 0.7971\n",
            "Epoch 12/25\n",
            "31/31 [==============================] - 12s 393ms/step - loss: 0.9581 - accuracy: 0.6842 - val_loss: 0.6502 - val_accuracy: 0.8059\n",
            "Epoch 13/25\n",
            "31/31 [==============================] - 12s 393ms/step - loss: 0.8877 - accuracy: 0.7075 - val_loss: 0.5136 - val_accuracy: 0.8382\n",
            "Epoch 14/25\n",
            "31/31 [==============================] - 12s 394ms/step - loss: 0.8238 - accuracy: 0.7358 - val_loss: 0.5595 - val_accuracy: 0.8324\n",
            "Epoch 15/25\n",
            "31/31 [==============================] - 12s 398ms/step - loss: 0.8241 - accuracy: 0.7328 - val_loss: 0.6164 - val_accuracy: 0.8000\n",
            "Epoch 16/25\n",
            "31/31 [==============================] - 12s 394ms/step - loss: 0.8104 - accuracy: 0.7470 - val_loss: 0.4835 - val_accuracy: 0.8412\n",
            "Epoch 17/25\n",
            "31/31 [==============================] - 12s 399ms/step - loss: 0.8302 - accuracy: 0.7500 - val_loss: 0.4465 - val_accuracy: 0.8647\n",
            "Epoch 18/25\n",
            "31/31 [==============================] - 12s 400ms/step - loss: 0.7887 - accuracy: 0.7490 - val_loss: 0.5186 - val_accuracy: 0.8471\n",
            "Epoch 19/25\n",
            "31/31 [==============================] - 12s 396ms/step - loss: 0.7384 - accuracy: 0.7571 - val_loss: 0.4151 - val_accuracy: 0.8647\n",
            "Epoch 20/25\n",
            "31/31 [==============================] - 12s 396ms/step - loss: 0.6997 - accuracy: 0.7682 - val_loss: 0.4422 - val_accuracy: 0.8588\n",
            "Epoch 21/25\n",
            "31/31 [==============================] - 12s 398ms/step - loss: 0.6711 - accuracy: 0.7763 - val_loss: 0.5340 - val_accuracy: 0.8206\n",
            "Epoch 22/25\n",
            "31/31 [==============================] - 12s 401ms/step - loss: 0.6689 - accuracy: 0.7844 - val_loss: 0.5371 - val_accuracy: 0.8294\n",
            "Epoch 23/25\n",
            "31/31 [==============================] - 12s 398ms/step - loss: 0.6131 - accuracy: 0.8006 - val_loss: 0.4030 - val_accuracy: 0.8676\n",
            "Epoch 24/25\n",
            "31/31 [==============================] - 12s 400ms/step - loss: 0.5960 - accuracy: 0.8148 - val_loss: 0.4583 - val_accuracy: 0.8824\n",
            "Epoch 25/25\n",
            "31/31 [==============================] - 12s 393ms/step - loss: 0.6805 - accuracy: 0.7844 - val_loss: 0.5206 - val_accuracy: 0.8324\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb9ebee3b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ec6yFiSF0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05115c1-2b94-4764-dd42-16a1d6d7a966"
      },
      "source": [
        "# evaluate the network after initialization\n",
        "print(\"[INFO] evaluating after initialization...\")\n",
        "predictions = model.predict(testX, batch_size=32)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=classNames))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating after initialization...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    bluebell       0.74      0.77      0.76        22\n",
            "   buttercup       0.93      0.78      0.85        18\n",
            "   coltsfoot       0.55      1.00      0.71        16\n",
            "     cowslip       0.75      0.52      0.62        23\n",
            "      crocus       0.85      0.96      0.90        23\n",
            "    daffodil       0.72      0.65      0.68        20\n",
            "       daisy       1.00      1.00      1.00        18\n",
            "   dandelion       0.85      0.73      0.79        15\n",
            "  fritillary       0.94      1.00      0.97        16\n",
            "        iris       0.94      0.85      0.89        20\n",
            "  lilyvalley       0.90      0.95      0.92        19\n",
            "       pansy       0.94      0.85      0.89        20\n",
            "    snowdrop       0.95      0.74      0.83        27\n",
            "   sunflower       0.82      1.00      0.90        23\n",
            "   tigerlily       0.95      1.00      0.98        21\n",
            "       tulip       0.64      0.47      0.55        19\n",
            "  windflower       0.79      0.95      0.86        20\n",
            "\n",
            "    accuracy                           0.83       340\n",
            "   macro avg       0.84      0.84      0.83       340\n",
            "weighted avg       0.84      0.83      0.83       340\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMPyOOx0SF0G"
      },
      "source": [
        "# now that the head FC layers have been trained/initialized, lets\n",
        "# unfreeze the final set of CONV layers and make them trainable\n",
        "for layer in baseModel.layers[15:]:\n",
        "\tlayer.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JINpOWoUSF0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6448e101-bdcd-4e94-ae5c-5cded312ef2e"
      },
      "source": [
        "# for the changes to the model to take affect we need to recompile\n",
        "# the model, this time using SGD with a *very* small learning rate\n",
        "print(\"[INFO] re-compiling model...\")\n",
        "opt = SGD(lr=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] re-compiling model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABz3PNjISF0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4326767-2fb3-41c3-f47e-fc36ac7615de"
      },
      "source": [
        "# train the model again, this time fine-tuning *both* the final set\n",
        "# of CONV layers along with our set of FC layers\n",
        "print(\"[INFO] fine-tuning model...\")\n",
        "model.fit_generator(aug.flow(trainX, trainY, batch_size=32),\n",
        "\tvalidation_data=(testX, testY), epochs=100,\n",
        "\tsteps_per_epoch=len(trainX) // 32, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] fine-tuning model...\n",
            "Epoch 1/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.4754 - accuracy: 0.8472 - val_loss: 0.3880 - val_accuracy: 0.8824\n",
            "Epoch 2/100\n",
            "31/31 [==============================] - 13s 404ms/step - loss: 0.4293 - accuracy: 0.8543 - val_loss: 0.3446 - val_accuracy: 0.8882\n",
            "Epoch 3/100\n",
            "31/31 [==============================] - 13s 407ms/step - loss: 0.3846 - accuracy: 0.8694 - val_loss: 0.4090 - val_accuracy: 0.8824\n",
            "Epoch 4/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.3634 - accuracy: 0.8725 - val_loss: 0.3562 - val_accuracy: 0.8941\n",
            "Epoch 5/100\n",
            "31/31 [==============================] - 12s 402ms/step - loss: 0.3517 - accuracy: 0.8826 - val_loss: 0.3632 - val_accuracy: 0.8971\n",
            "Epoch 6/100\n",
            "31/31 [==============================] - 12s 401ms/step - loss: 0.3574 - accuracy: 0.8745 - val_loss: 0.3267 - val_accuracy: 0.9029\n",
            "Epoch 7/100\n",
            "31/31 [==============================] - 12s 402ms/step - loss: 0.3639 - accuracy: 0.8765 - val_loss: 0.3114 - val_accuracy: 0.9059\n",
            "Epoch 8/100\n",
            "31/31 [==============================] - 13s 408ms/step - loss: 0.3145 - accuracy: 0.9089 - val_loss: 0.3236 - val_accuracy: 0.9088\n",
            "Epoch 9/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.3084 - accuracy: 0.8978 - val_loss: 0.3897 - val_accuracy: 0.8882\n",
            "Epoch 10/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.3054 - accuracy: 0.8897 - val_loss: 0.3284 - val_accuracy: 0.9059\n",
            "Epoch 11/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.2993 - accuracy: 0.9008 - val_loss: 0.3676 - val_accuracy: 0.9029\n",
            "Epoch 12/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.3122 - accuracy: 0.8897 - val_loss: 0.3357 - val_accuracy: 0.9059\n",
            "Epoch 13/100\n",
            "31/31 [==============================] - 13s 405ms/step - loss: 0.2977 - accuracy: 0.8846 - val_loss: 0.3227 - val_accuracy: 0.9029\n",
            "Epoch 14/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.2866 - accuracy: 0.9018 - val_loss: 0.3300 - val_accuracy: 0.9059\n",
            "Epoch 15/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.2911 - accuracy: 0.9012 - val_loss: 0.3058 - val_accuracy: 0.9059\n",
            "Epoch 16/100\n",
            "31/31 [==============================] - 13s 407ms/step - loss: 0.2810 - accuracy: 0.9028 - val_loss: 0.3297 - val_accuracy: 0.9059\n",
            "Epoch 17/100\n",
            "31/31 [==============================] - 13s 408ms/step - loss: 0.2692 - accuracy: 0.9150 - val_loss: 0.3054 - val_accuracy: 0.9147\n",
            "Epoch 18/100\n",
            "31/31 [==============================] - 13s 403ms/step - loss: 0.2592 - accuracy: 0.9049 - val_loss: 0.3202 - val_accuracy: 0.9147\n",
            "Epoch 19/100\n",
            "31/31 [==============================] - 13s 406ms/step - loss: 0.2661 - accuracy: 0.9160 - val_loss: 0.3563 - val_accuracy: 0.9118\n",
            "Epoch 20/100\n",
            "31/31 [==============================] - 13s 406ms/step - loss: 0.2742 - accuracy: 0.9109 - val_loss: 0.3235 - val_accuracy: 0.9118\n",
            "Epoch 21/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.2777 - accuracy: 0.9109 - val_loss: 0.3244 - val_accuracy: 0.9206\n",
            "Epoch 22/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.2014 - accuracy: 0.9261 - val_loss: 0.3214 - val_accuracy: 0.9059\n",
            "Epoch 23/100\n",
            "31/31 [==============================] - 13s 408ms/step - loss: 0.2677 - accuracy: 0.9130 - val_loss: 0.3263 - val_accuracy: 0.9059\n",
            "Epoch 24/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.2240 - accuracy: 0.9231 - val_loss: 0.3547 - val_accuracy: 0.9088\n",
            "Epoch 25/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.2756 - accuracy: 0.9089 - val_loss: 0.3267 - val_accuracy: 0.9088\n",
            "Epoch 26/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.2441 - accuracy: 0.9099 - val_loss: 0.3234 - val_accuracy: 0.9147\n",
            "Epoch 27/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.2501 - accuracy: 0.9119 - val_loss: 0.3223 - val_accuracy: 0.9118\n",
            "Epoch 28/100\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 0.2708 - accuracy: 0.9089 - val_loss: 0.3071 - val_accuracy: 0.9147\n",
            "Epoch 29/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.2138 - accuracy: 0.9271 - val_loss: 0.3217 - val_accuracy: 0.9000\n",
            "Epoch 30/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.2637 - accuracy: 0.9018 - val_loss: 0.2957 - val_accuracy: 0.9235\n",
            "Epoch 31/100\n",
            "31/31 [==============================] - 13s 408ms/step - loss: 0.2362 - accuracy: 0.9251 - val_loss: 0.3390 - val_accuracy: 0.9059\n",
            "Epoch 32/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.2173 - accuracy: 0.9221 - val_loss: 0.2895 - val_accuracy: 0.9147\n",
            "Epoch 33/100\n",
            "31/31 [==============================] - 13s 407ms/step - loss: 0.2011 - accuracy: 0.9352 - val_loss: 0.2982 - val_accuracy: 0.9235\n",
            "Epoch 34/100\n",
            "31/31 [==============================] - 13s 407ms/step - loss: 0.2042 - accuracy: 0.9271 - val_loss: 0.3043 - val_accuracy: 0.9206\n",
            "Epoch 35/100\n",
            "31/31 [==============================] - 13s 405ms/step - loss: 0.2286 - accuracy: 0.9211 - val_loss: 0.3003 - val_accuracy: 0.9147\n",
            "Epoch 36/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.2285 - accuracy: 0.9241 - val_loss: 0.2967 - val_accuracy: 0.9265\n",
            "Epoch 37/100\n",
            "31/31 [==============================] - 13s 406ms/step - loss: 0.1860 - accuracy: 0.9271 - val_loss: 0.3043 - val_accuracy: 0.9265\n",
            "Epoch 38/100\n",
            "31/31 [==============================] - 13s 404ms/step - loss: 0.2152 - accuracy: 0.9251 - val_loss: 0.2853 - val_accuracy: 0.9176\n",
            "Epoch 39/100\n",
            "31/31 [==============================] - 13s 405ms/step - loss: 0.2002 - accuracy: 0.9332 - val_loss: 0.3052 - val_accuracy: 0.9235\n",
            "Epoch 40/100\n",
            "31/31 [==============================] - 13s 407ms/step - loss: 0.2379 - accuracy: 0.9109 - val_loss: 0.3144 - val_accuracy: 0.9176\n",
            "Epoch 41/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.2309 - accuracy: 0.9231 - val_loss: 0.2963 - val_accuracy: 0.9118\n",
            "Epoch 42/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.2139 - accuracy: 0.9281 - val_loss: 0.2839 - val_accuracy: 0.9206\n",
            "Epoch 43/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.1969 - accuracy: 0.9251 - val_loss: 0.2912 - val_accuracy: 0.9147\n",
            "Epoch 44/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.2202 - accuracy: 0.9251 - val_loss: 0.2744 - val_accuracy: 0.9176\n",
            "Epoch 45/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.2431 - accuracy: 0.9190 - val_loss: 0.3145 - val_accuracy: 0.9206\n",
            "Epoch 46/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.1969 - accuracy: 0.9413 - val_loss: 0.3156 - val_accuracy: 0.9147\n",
            "Epoch 47/100\n",
            "31/31 [==============================] - 13s 408ms/step - loss: 0.1977 - accuracy: 0.9312 - val_loss: 0.3005 - val_accuracy: 0.9176\n",
            "Epoch 48/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.2137 - accuracy: 0.9261 - val_loss: 0.2914 - val_accuracy: 0.9206\n",
            "Epoch 49/100\n",
            "31/31 [==============================] - 13s 408ms/step - loss: 0.1740 - accuracy: 0.9453 - val_loss: 0.3005 - val_accuracy: 0.9206\n",
            "Epoch 50/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.1692 - accuracy: 0.9383 - val_loss: 0.3079 - val_accuracy: 0.9176\n",
            "Epoch 51/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.1519 - accuracy: 0.9443 - val_loss: 0.2947 - val_accuracy: 0.9235\n",
            "Epoch 52/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.1773 - accuracy: 0.9365 - val_loss: 0.3085 - val_accuracy: 0.9176\n",
            "Epoch 53/100\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 0.1880 - accuracy: 0.9342 - val_loss: 0.2904 - val_accuracy: 0.9206\n",
            "Epoch 54/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.1841 - accuracy: 0.9393 - val_loss: 0.2944 - val_accuracy: 0.9294\n",
            "Epoch 55/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.1845 - accuracy: 0.9322 - val_loss: 0.2703 - val_accuracy: 0.9235\n",
            "Epoch 56/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.1581 - accuracy: 0.9504 - val_loss: 0.2806 - val_accuracy: 0.9147\n",
            "Epoch 57/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.1846 - accuracy: 0.9342 - val_loss: 0.2840 - val_accuracy: 0.9265\n",
            "Epoch 58/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.1532 - accuracy: 0.9474 - val_loss: 0.2874 - val_accuracy: 0.9235\n",
            "Epoch 59/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.1533 - accuracy: 0.9464 - val_loss: 0.2996 - val_accuracy: 0.9265\n",
            "Epoch 60/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.1471 - accuracy: 0.9514 - val_loss: 0.2967 - val_accuracy: 0.9235\n",
            "Epoch 61/100\n",
            "31/31 [==============================] - 13s 409ms/step - loss: 0.2035 - accuracy: 0.9281 - val_loss: 0.2812 - val_accuracy: 0.9235\n",
            "Epoch 62/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.1641 - accuracy: 0.9362 - val_loss: 0.2767 - val_accuracy: 0.9353\n",
            "Epoch 63/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.2063 - accuracy: 0.9332 - val_loss: 0.2738 - val_accuracy: 0.9294\n",
            "Epoch 64/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.1735 - accuracy: 0.9403 - val_loss: 0.2851 - val_accuracy: 0.9294\n",
            "Epoch 65/100\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 0.1677 - accuracy: 0.9352 - val_loss: 0.2752 - val_accuracy: 0.9324\n",
            "Epoch 66/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.1721 - accuracy: 0.9342 - val_loss: 0.2695 - val_accuracy: 0.9294\n",
            "Epoch 67/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.1732 - accuracy: 0.9372 - val_loss: 0.2681 - val_accuracy: 0.9235\n",
            "Epoch 68/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.1836 - accuracy: 0.9383 - val_loss: 0.2720 - val_accuracy: 0.9235\n",
            "Epoch 69/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.1662 - accuracy: 0.9464 - val_loss: 0.2775 - val_accuracy: 0.9324\n",
            "Epoch 70/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.1655 - accuracy: 0.9403 - val_loss: 0.2824 - val_accuracy: 0.9265\n",
            "Epoch 71/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.1636 - accuracy: 0.9433 - val_loss: 0.2716 - val_accuracy: 0.9294\n",
            "Epoch 72/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.1358 - accuracy: 0.9575 - val_loss: 0.2676 - val_accuracy: 0.9324\n",
            "Epoch 73/100\n",
            "31/31 [==============================] - 13s 408ms/step - loss: 0.1856 - accuracy: 0.9393 - val_loss: 0.2796 - val_accuracy: 0.9294\n",
            "Epoch 74/100\n",
            "31/31 [==============================] - 13s 410ms/step - loss: 0.1804 - accuracy: 0.9393 - val_loss: 0.2884 - val_accuracy: 0.9206\n",
            "Epoch 75/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.1410 - accuracy: 0.9534 - val_loss: 0.2809 - val_accuracy: 0.9382\n",
            "Epoch 76/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.1527 - accuracy: 0.9443 - val_loss: 0.2751 - val_accuracy: 0.9294\n",
            "Epoch 77/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.1598 - accuracy: 0.9413 - val_loss: 0.2647 - val_accuracy: 0.9235\n",
            "Epoch 78/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.1503 - accuracy: 0.9443 - val_loss: 0.2706 - val_accuracy: 0.9294\n",
            "Epoch 79/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.1262 - accuracy: 0.9615 - val_loss: 0.3009 - val_accuracy: 0.9265\n",
            "Epoch 80/100\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 0.1538 - accuracy: 0.9453 - val_loss: 0.2485 - val_accuracy: 0.9353\n",
            "Epoch 81/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.1606 - accuracy: 0.9464 - val_loss: 0.2725 - val_accuracy: 0.9382\n",
            "Epoch 82/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.1497 - accuracy: 0.9484 - val_loss: 0.2631 - val_accuracy: 0.9265\n",
            "Epoch 83/100\n",
            "31/31 [==============================] - 13s 418ms/step - loss: 0.1602 - accuracy: 0.9425 - val_loss: 0.2542 - val_accuracy: 0.9324\n",
            "Epoch 84/100\n",
            "31/31 [==============================] - 13s 411ms/step - loss: 0.1325 - accuracy: 0.9534 - val_loss: 0.2551 - val_accuracy: 0.9265\n",
            "Epoch 85/100\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 0.1363 - accuracy: 0.9524 - val_loss: 0.2388 - val_accuracy: 0.9382\n",
            "Epoch 86/100\n",
            "31/31 [==============================] - 13s 417ms/step - loss: 0.1367 - accuracy: 0.9565 - val_loss: 0.2724 - val_accuracy: 0.9265\n",
            "Epoch 87/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.1315 - accuracy: 0.9524 - val_loss: 0.2303 - val_accuracy: 0.9324\n",
            "Epoch 88/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.1306 - accuracy: 0.9555 - val_loss: 0.2624 - val_accuracy: 0.9294\n",
            "Epoch 89/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.1402 - accuracy: 0.9494 - val_loss: 0.2574 - val_accuracy: 0.9382\n",
            "Epoch 90/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.1451 - accuracy: 0.9514 - val_loss: 0.2650 - val_accuracy: 0.9412\n",
            "Epoch 91/100\n",
            "31/31 [==============================] - 13s 416ms/step - loss: 0.1104 - accuracy: 0.9587 - val_loss: 0.2571 - val_accuracy: 0.9324\n",
            "Epoch 92/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.1353 - accuracy: 0.9494 - val_loss: 0.2580 - val_accuracy: 0.9353\n",
            "Epoch 93/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.1183 - accuracy: 0.9605 - val_loss: 0.2530 - val_accuracy: 0.9324\n",
            "Epoch 94/100\n",
            "31/31 [==============================] - 13s 413ms/step - loss: 0.1211 - accuracy: 0.9585 - val_loss: 0.2614 - val_accuracy: 0.9294\n",
            "Epoch 95/100\n",
            "31/31 [==============================] - 13s 416ms/step - loss: 0.1435 - accuracy: 0.9536 - val_loss: 0.2658 - val_accuracy: 0.9235\n",
            "Epoch 96/100\n",
            "31/31 [==============================] - 13s 414ms/step - loss: 0.1300 - accuracy: 0.9524 - val_loss: 0.2442 - val_accuracy: 0.9265\n",
            "Epoch 97/100\n",
            "31/31 [==============================] - 13s 412ms/step - loss: 0.1306 - accuracy: 0.9585 - val_loss: 0.2479 - val_accuracy: 0.9353\n",
            "Epoch 98/100\n",
            "31/31 [==============================] - 13s 419ms/step - loss: 0.1246 - accuracy: 0.9575 - val_loss: 0.2373 - val_accuracy: 0.9441\n",
            "Epoch 99/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.1293 - accuracy: 0.9636 - val_loss: 0.2865 - val_accuracy: 0.9324\n",
            "Epoch 100/100\n",
            "31/31 [==============================] - 13s 415ms/step - loss: 0.1335 - accuracy: 0.9595 - val_loss: 0.2627 - val_accuracy: 0.9265\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbae0061668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-ZyhfvYSF0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348fc41d-f477-40fa-d098-62b40f067dd5"
      },
      "source": [
        "# evaluate the network on the fine-tuned model\n",
        "print(\"[INFO] evaluating after fine-tuning...\")\n",
        "predictions = model.predict(testX, batch_size=32)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=classNames))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating after fine-tuning...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    bluebell       0.86      0.86      0.86        22\n",
            "   buttercup       0.95      1.00      0.97        18\n",
            "   coltsfoot       0.94      1.00      0.97        16\n",
            "     cowslip       0.91      0.87      0.89        23\n",
            "      crocus       1.00      0.96      0.98        23\n",
            "    daffodil       0.83      1.00      0.91        20\n",
            "       daisy       1.00      1.00      1.00        18\n",
            "   dandelion       1.00      0.93      0.97        15\n",
            "  fritillary       0.94      1.00      0.97        16\n",
            "        iris       0.94      0.85      0.89        20\n",
            "  lilyvalley       0.90      0.95      0.92        19\n",
            "       pansy       0.94      0.85      0.89        20\n",
            "    snowdrop       0.93      0.93      0.93        27\n",
            "   sunflower       1.00      0.91      0.95        23\n",
            "   tigerlily       1.00      0.95      0.98        21\n",
            "       tulip       0.79      0.79      0.79        19\n",
            "  windflower       0.86      0.95      0.90        20\n",
            "\n",
            "    accuracy                           0.93       340\n",
            "   macro avg       0.93      0.93      0.93       340\n",
            "weighted avg       0.93      0.93      0.93       340\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}