{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "train_alexnet_dogsvscats.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sripraks/CNN/blob/main/train_alexnet_dogsvscats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OZgrKCEeB9n"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "class ImageToArrayPreprocessor:\n",
        "\tdef __init__(self, dataFormat=None):\n",
        "\t\t# store the image data format\n",
        "\t\tself.dataFormat = dataFormat\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# apply the Keras utility function that correctly rearranges\n",
        "\t\t# the dimensions of the image\n",
        "\t\treturn img_to_array(image, data_format=self.dataFormat)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iU6wirFeHXF"
      },
      "source": [
        "# import the necessary packages\n",
        "import cv2\n",
        "\n",
        "class SimplePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# resize the image to a fixed size, ignoring the aspect\n",
        "\t\t# ratio\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msG4Gp0feOVv"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.feature_extraction.image import extract_patches_2d\n",
        "\n",
        "class PatchPreprocessor:\n",
        "\tdef __init__(self, width, height):\n",
        "\t\t# store the target width and height of the image\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# extract a random crop from the image with the target width\n",
        "\t\t# and height\n",
        "\t\treturn extract_patches_2d(image, (self.height, self.width),\n",
        "\t\t\tmax_patches=1)[0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6rrDwhqeUme"
      },
      "source": [
        "# import the necessary packages\n",
        "import cv2\n",
        "\n",
        "class MeanPreprocessor:\n",
        "\tdef __init__(self, rMean, gMean, bMean):\n",
        "\t\t# store the Red, Green, and Blue channel averages across a\n",
        "\t\t# training set\n",
        "\t\tself.rMean = rMean\n",
        "\t\tself.gMean = gMean\n",
        "\t\tself.bMean = bMean\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# split the image into its respective Red, Green, and Blue\n",
        "\t\t# channels\n",
        "\t\t(B, G, R) = cv2.split(image.astype(\"float32\"))\n",
        "\n",
        "\t\t# subtract the means for each channel\n",
        "\t\tR -= self.rMean\n",
        "\t\tG -= self.gMean\n",
        "\t\tB -= self.bMean\n",
        "\n",
        "\t\t# merge the channels back together and return the image\n",
        "\t\treturn cv2.merge([B, G, R])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKyVuTs8ebZQ"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.callbacks import BaseLogger\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "class TrainingMonitor(BaseLogger):\n",
        "\tdef __init__(self, figPath, jsonPath=None, startAt=0):\n",
        "\t\t# store the output path for the figure, the path to the JSON\n",
        "\t\t# serialized file, and the starting epoch\n",
        "\t\tsuper(TrainingMonitor, self).__init__()\n",
        "\t\tself.figPath = figPath\n",
        "\t\tself.jsonPath = jsonPath\n",
        "\t\tself.startAt = startAt\n",
        "\n",
        "\tdef on_train_begin(self, logs={}):\n",
        "\t\t# initialize the history dictionary\n",
        "\t\tself.H = {}\n",
        "\n",
        "\t\t# if the JSON history path exists, load the training history\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tif os.path.exists(self.jsonPath):\n",
        "\t\t\t\tself.H = json.loads(open(self.jsonPath).read())\n",
        "\n",
        "\t\t\t\t# check to see if a starting epoch was supplied\n",
        "\t\t\t\tif self.startAt > 0:\n",
        "\t\t\t\t\t# loop over the entries in the history log and\n",
        "\t\t\t\t\t# trim any entries that are past the starting\n",
        "\t\t\t\t\t# epoch\n",
        "\t\t\t\t\tfor k in self.H.keys():\n",
        "\t\t\t\t\t\tself.H[k] = self.H[k][:self.startAt]\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# loop over the logs and update the loss, accuracy, etc.\n",
        "\t\t# for the entire training process\n",
        "\t\tfor (k, v) in logs.items():\n",
        "\t\t\tl = self.H.get(k, [])\n",
        "\t\t\tl.append(float(v))\n",
        "\t\t\tself.H[k] = l\n",
        "\n",
        "\t\t# check to see if the training history should be serialized\n",
        "\t\t# to file\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tf = open(self.jsonPath, \"w\")\n",
        "\t\t\tf.write(json.dumps(self.H))\n",
        "\t\t\tf.close()\n",
        "\n",
        "\t\t# ensure at least two epochs have passed before plotting\n",
        "\t\t# (epoch starts at zero)\n",
        "\t\tif len(self.H[\"loss\"]) > 1:\n",
        "\t\t\t# plot the training loss and accuracy\n",
        "\t\t\tN = np.arange(0, len(self.H[\"loss\"]))\n",
        "\t\t\tplt.style.use(\"ggplot\")\n",
        "\t\t\tplt.figure()\n",
        "\t\t\tplt.plot(N, self.H[\"loss\"], label=\"train_loss\")\n",
        "\t\t\tplt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\n",
        "\t\t\tplt.plot(N, self.H[\"accuracy\"], label=\"train_acc\")\n",
        "\t\t\tplt.plot(N, self.H[\"val_accuracy\"], label=\"val_acc\")\n",
        "\t\t\tplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(\n",
        "\t\t\t\tlen(self.H[\"loss\"])))\n",
        "\t\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\t\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\t\t\tplt.legend()\n",
        "\n",
        "\t\t\t# save the figure\n",
        "\t\t\tplt.savefig(self.figPath)\n",
        "\t\t\tplt.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xztrLlVBejp_"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "class HDF5DatasetGenerator:\n",
        "\tdef __init__(self, dbPath, batchSize, preprocessors=None,\n",
        "\t\taug=None, binarize=True, classes=2):\n",
        "\t\t# store the batch size, preprocessors, and data augmentor,\n",
        "\t\t# whether or not the labels should be binarized, along with\n",
        "\t\t# the total number of classes\n",
        "\t\tself.batchSize = batchSize\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\t\tself.aug = aug\n",
        "\t\tself.binarize = binarize\n",
        "\t\tself.classes = classes\n",
        "\n",
        "\t\t# open the HDF5 database for reading and determine the total\n",
        "\t\t# number of entries in the database\n",
        "\t\tself.db = h5py.File(dbPath, \"r\")\n",
        "\t\tself.numImages = self.db[\"labels\"].shape[0]\n",
        "\n",
        "\tdef generator(self, passes=np.inf):\n",
        "\t\t# initialize the epoch count\n",
        "\t\tepochs = 0\n",
        "\n",
        "\t\t# keep looping infinitely -- the model will stop once we have\n",
        "\t\t# reach the desired number of epochs\n",
        "\t\twhile epochs < passes:\n",
        "\t\t\t# loop over the HDF5 dataset\n",
        "\t\t\tfor i in np.arange(0, self.numImages, self.batchSize):\n",
        "\t\t\t\t# extract the images and labels from the HDF dataset\n",
        "\t\t\t\timages = self.db[\"images\"][i: i + self.batchSize]\n",
        "\t\t\t\tlabels = self.db[\"labels\"][i: i + self.batchSize]\n",
        "\n",
        "\t\t\t\t# check to see if the labels should be binarized\n",
        "\t\t\t\tif self.binarize:\n",
        "\t\t\t\t\tlabels = to_categorical(labels,\n",
        "\t\t\t\t\t\tself.classes)\n",
        "\n",
        "\t\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t\t# initialize the list of processed images\n",
        "\t\t\t\t\tprocImages = []\n",
        "\n",
        "\t\t\t\t\t# loop over the images\n",
        "\t\t\t\t\tfor image in images:\n",
        "\t\t\t\t\t\t# loop over the preprocessors and apply each\n",
        "\t\t\t\t\t\t# to the image\n",
        "\t\t\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t\t\t\t# update the list of processed images\n",
        "\t\t\t\t\t\tprocImages.append(image)\n",
        "\n",
        "\t\t\t\t\t# update the images array to be the processed\n",
        "\t\t\t\t\t# images\n",
        "\t\t\t\t\timages = np.array(procImages)\n",
        "\n",
        "\t\t\t\t# if the data augmenator exists, apply it\n",
        "\t\t\t\tif self.aug is not None:\n",
        "\t\t\t\t\t(images, labels) = next(self.aug.flow(images,\n",
        "\t\t\t\t\t\tlabels, batch_size=self.batchSize))\n",
        "\n",
        "\t\t\t\t# yield a tuple of images and labels\n",
        "\t\t\t\tyield (images, labels)\n",
        "\n",
        "\t\t\t# increment the total number of epochs\n",
        "\t\t\tepochs += 1\n",
        "\n",
        "\tdef close(self):\n",
        "\t\t# close the database\n",
        "\t\tself.db.close()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN101glKepcU"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class AlexNet:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes, reg=0.0002):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "\t\t# Block #1: first CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(96, (11, 11), strides=(4, 4),\n",
        "\t\t\tinput_shape=inputShape, padding=\"same\",\n",
        "\t\t\tkernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #2: second CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(256, (5, 5), padding=\"same\",\n",
        "\t\t\tkernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #3: CONV => RELU => CONV => RELU => CONV => RELU\n",
        "\t\tmodel.add(Conv2D(384, (3, 3), padding=\"same\",\n",
        "\t\t\tkernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(384, (3, 3), padding=\"same\",\n",
        "\t\t\tkernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(256, (3, 3), padding=\"same\",\n",
        "\t\t\tkernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #4: first set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(4096, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\n",
        "\t\t# Block #5: second set of FC => RELU layers\n",
        "\t\tmodel.add(Dense(4096, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\n",
        "\t\t# softmax classifier\n",
        "\t\tmodel.add(Dense(classes, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"softmax\"))\n",
        "\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6DlgHrXe7Pv"
      },
      "source": [
        "# define the paths to the images directory\n",
        "IMAGES_PATH = \"/content/drive/MyDrive/dataset/catsanddogs/train\"\n",
        "\n",
        "# since we do not have validation data or access to the testing\n",
        "# labels we need to take a number of images from the training\n",
        "# data and use them instead\n",
        "NUM_CLASSES = 2\n",
        "NUM_VAL_IMAGES = 100 * NUM_CLASSES\n",
        "NUM_TEST_IMAGES = 100 * NUM_CLASSES\n",
        "\n",
        "# define the path to the output training, validation, and testing\n",
        "# HDF5 files\n",
        "TRAIN_HDF5 = \"/content/drive/MyDrive/dataset/catsanddogs/train/hdf5/train.hdf5\"\n",
        "VAL_HDF5 = \"/content/drive/MyDrive/dataset/catsanddogs/train/hdf5/val.hdf5\"\n",
        "TEST_HDF5 = \"/content/drive/MyDrive/dataset/catsanddogs/train/hdf5/test.hdf5\"\n",
        "\n",
        "# path to the output model file\n",
        "MODEL_PATH = \"/content/drive/MyDrive/dataset/catsanddogs/train/output/alexnet_dogs_vs_cats.model\"\n",
        "\n",
        "# define the path to the dataset mean\n",
        "DATASET_MEAN = \"/content/drive/MyDrive/dataset/catsanddogs/train/output/dogs_vs_cats_mean.json\"\n",
        "\n",
        "# define the path to the output directory used for storing plots,\n",
        "# classification reports, etc.\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/dataset/catsanddogs/train/output\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e4Nk-fRdpyB"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import json\n",
        "import os"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeiEsDvhdpyC"
      },
      "source": [
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,horizontal_flip=True, fill_mode=\"nearest\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB-GYum7fp3I",
        "outputId": "f87d0e63-816a-4469-da8b-cbb5153b1f7e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnbBFWtMdpyC"
      },
      "source": [
        "# load the RGB means for the training set\n",
        "means = json.loads(open(DATASET_MEAN).read())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJK0x4ucdpyC",
        "outputId": "1f66d0c9-d456-4ac5-849c-623401331df7"
      },
      "source": [
        "print(means)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'R': 124.81371888388111, 'G': 115.51942131432999, 'B': 105.85957157358844}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDGTyjhrdpyD"
      },
      "source": [
        "# initialize the image preprocessors\n",
        "sp = SimplePreprocessor(227, 227)\n",
        "pp = PatchPreprocessor(227, 227)\n",
        "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
        "iap = ImageToArrayPreprocessor()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JEAZ18qdpyD"
      },
      "source": [
        "# initialize the training and validation dataset generators\n",
        "trainGen = HDF5DatasetGenerator(TRAIN_HDF5, 128, aug=aug,preprocessors=[pp, mp, iap], classes=2)\n",
        "valGen = HDF5DatasetGenerator(VAL_HDF5, 128,preprocessors=[sp, mp, iap], classes=2)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZfA1hahdpyD",
        "outputId": "c1564041-6aee-40ea-8c74-12ab6cb75e8a"
      },
      "source": [
        "# initialize the optimizer\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=1e-3)\n",
        "model = AlexNet.build(width=227, height=227, depth=3,classes=2, reg=0.0002)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJY57qNAdpyD"
      },
      "source": [
        "# construct the set of callbacks\n",
        "path = os.path.sep.join([OUTPUT_PATH, \"{}.png\".format(os.getpid())])\n",
        "callbacks = [TrainingMonitor(path)]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBdaHGnrdpyD",
        "outputId": "4932bc8a-e073-4281-a476-f73418468337"
      },
      "source": [
        "# train the network\n",
        "model.fit_generator(\n",
        "\ttrainGen.generator(),\n",
        "\tsteps_per_epoch=trainGen.numImages // 128,\n",
        "\tvalidation_data=valGen.generator(),\n",
        "\tvalidation_steps=valGen.numImages // 128,\n",
        "\tepochs=75,\n",
        "\tmax_queue_size=10,\n",
        "\tcallbacks=callbacks, verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-22-a55f36e24c44>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/75\n",
            "12/12 [==============================] - 33s 3s/step - loss: 7.2516 - accuracy: 0.5000 - val_loss: 10.5056 - val_accuracy: 0.4688\n",
            "Epoch 2/75\n",
            "12/12 [==============================] - 60s 5s/step - loss: 4.0512 - accuracy: 0.5597 - val_loss: 10.6241 - val_accuracy: 0.4688\n",
            "Epoch 3/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 3.6813 - accuracy: 0.5543 - val_loss: 10.6678 - val_accuracy: 0.4688\n",
            "Epoch 4/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 3.4517 - accuracy: 0.5672 - val_loss: 10.6643 - val_accuracy: 0.4688\n",
            "Epoch 5/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 3.4105 - accuracy: 0.5699 - val_loss: 6.2907 - val_accuracy: 0.4531\n",
            "Epoch 6/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 3.4294 - accuracy: 0.5475 - val_loss: 10.6037 - val_accuracy: 0.4688\n",
            "Epoch 7/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 3.2842 - accuracy: 0.5997 - val_loss: 9.2784 - val_accuracy: 0.4688\n",
            "Epoch 8/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 3.2701 - accuracy: 0.6194 - val_loss: 9.5450 - val_accuracy: 0.4688\n",
            "Epoch 9/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 3.2361 - accuracy: 0.5760 - val_loss: 8.2411 - val_accuracy: 0.4844\n",
            "Epoch 10/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 3.1824 - accuracy: 0.6038 - val_loss: 5.5717 - val_accuracy: 0.4766\n",
            "Epoch 11/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 3.0958 - accuracy: 0.6275 - val_loss: 4.8672 - val_accuracy: 0.5078\n",
            "Epoch 12/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.9818 - accuracy: 0.6567 - val_loss: 4.8053 - val_accuracy: 0.5547\n",
            "Epoch 13/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.9535 - accuracy: 0.6269 - val_loss: 3.0287 - val_accuracy: 0.6016\n",
            "Epoch 14/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.8885 - accuracy: 0.6419 - val_loss: 3.0931 - val_accuracy: 0.6250\n",
            "Epoch 15/75\n",
            "12/12 [==============================] - 17s 1s/step - loss: 2.8741 - accuracy: 0.6486 - val_loss: 3.4273 - val_accuracy: 0.5234\n",
            "Epoch 16/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.8013 - accuracy: 0.6608 - val_loss: 2.9225 - val_accuracy: 0.6406\n",
            "Epoch 17/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.8001 - accuracy: 0.6947 - val_loss: 2.6954 - val_accuracy: 0.6562\n",
            "Epoch 18/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.8379 - accuracy: 0.6248 - val_loss: 2.7164 - val_accuracy: 0.6719\n",
            "Epoch 19/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.7338 - accuracy: 0.6465 - val_loss: 3.0113 - val_accuracy: 0.5938\n",
            "Epoch 20/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.6835 - accuracy: 0.6628 - val_loss: 2.7415 - val_accuracy: 0.6250\n",
            "Epoch 21/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.6652 - accuracy: 0.6452 - val_loss: 2.6401 - val_accuracy: 0.6562\n",
            "Epoch 22/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.6258 - accuracy: 0.6588 - val_loss: 2.9727 - val_accuracy: 0.5625\n",
            "Epoch 23/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.6470 - accuracy: 0.6493 - val_loss: 2.5577 - val_accuracy: 0.6406\n",
            "Epoch 24/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.5503 - accuracy: 0.6615 - val_loss: 2.5992 - val_accuracy: 0.6250\n",
            "Epoch 25/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.4090 - accuracy: 0.7144 - val_loss: 2.4471 - val_accuracy: 0.6016\n",
            "Epoch 26/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.3690 - accuracy: 0.6967 - val_loss: 2.4080 - val_accuracy: 0.6406\n",
            "Epoch 27/75\n",
            "12/12 [==============================] - 17s 1s/step - loss: 2.3480 - accuracy: 0.6966 - val_loss: 2.5300 - val_accuracy: 0.6094\n",
            "Epoch 28/75\n",
            "12/12 [==============================] - 17s 1s/step - loss: 2.3077 - accuracy: 0.7022 - val_loss: 2.4923 - val_accuracy: 0.6094\n",
            "Epoch 29/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.2800 - accuracy: 0.7130 - val_loss: 2.6429 - val_accuracy: 0.5938\n",
            "Epoch 30/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.2089 - accuracy: 0.7110 - val_loss: 2.3689 - val_accuracy: 0.6406\n",
            "Epoch 31/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.2852 - accuracy: 0.7090 - val_loss: 2.2007 - val_accuracy: 0.7031\n",
            "Epoch 32/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.1258 - accuracy: 0.7185 - val_loss: 2.0767 - val_accuracy: 0.6797\n",
            "Epoch 33/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.1120 - accuracy: 0.7137 - val_loss: 2.3925 - val_accuracy: 0.6406\n",
            "Epoch 34/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.1204 - accuracy: 0.7218 - val_loss: 3.0777 - val_accuracy: 0.6406\n",
            "Epoch 35/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 2.0628 - accuracy: 0.7198 - val_loss: 1.9837 - val_accuracy: 0.7344\n",
            "Epoch 36/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.9955 - accuracy: 0.7347 - val_loss: 2.1629 - val_accuracy: 0.6875\n",
            "Epoch 37/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.9516 - accuracy: 0.7415 - val_loss: 1.9381 - val_accuracy: 0.6953\n",
            "Epoch 38/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.9027 - accuracy: 0.7280 - val_loss: 2.1534 - val_accuracy: 0.5859\n",
            "Epoch 39/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.8494 - accuracy: 0.7463 - val_loss: 2.0636 - val_accuracy: 0.6484\n",
            "Epoch 40/75\n",
            "12/12 [==============================] - 17s 1s/step - loss: 1.8444 - accuracy: 0.7604 - val_loss: 1.9758 - val_accuracy: 0.6328\n",
            "Epoch 41/75\n",
            "12/12 [==============================] - 17s 1s/step - loss: 1.8229 - accuracy: 0.7503 - val_loss: 2.1845 - val_accuracy: 0.6250\n",
            "Epoch 42/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.7555 - accuracy: 0.7646 - val_loss: 1.9217 - val_accuracy: 0.6484\n",
            "Epoch 43/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.7358 - accuracy: 0.7598 - val_loss: 1.9115 - val_accuracy: 0.6328\n",
            "Epoch 44/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.6444 - accuracy: 0.7775 - val_loss: 1.8415 - val_accuracy: 0.6484\n",
            "Epoch 45/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.6251 - accuracy: 0.7877 - val_loss: 2.2035 - val_accuracy: 0.5781\n",
            "Epoch 46/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.5963 - accuracy: 0.7843 - val_loss: 1.9278 - val_accuracy: 0.6953\n",
            "Epoch 47/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.6223 - accuracy: 0.7673 - val_loss: 1.8127 - val_accuracy: 0.6484\n",
            "Epoch 48/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.5848 - accuracy: 0.7646 - val_loss: 2.0268 - val_accuracy: 0.6562\n",
            "Epoch 49/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.5517 - accuracy: 0.7815 - val_loss: 1.8034 - val_accuracy: 0.6484\n",
            "Epoch 50/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.5549 - accuracy: 0.7768 - val_loss: 1.6093 - val_accuracy: 0.7188\n",
            "Epoch 51/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.6585 - accuracy: 0.7334 - val_loss: 2.1506 - val_accuracy: 0.5781\n",
            "Epoch 52/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.5728 - accuracy: 0.7578 - val_loss: 1.9107 - val_accuracy: 0.6562\n",
            "Epoch 53/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.5607 - accuracy: 0.7513 - val_loss: 1.5907 - val_accuracy: 0.6719\n",
            "Epoch 54/75\n",
            "12/12 [==============================] - 17s 1s/step - loss: 1.4932 - accuracy: 0.7659 - val_loss: 1.7911 - val_accuracy: 0.6406\n",
            "Epoch 55/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.5047 - accuracy: 0.7754 - val_loss: 2.6842 - val_accuracy: 0.5938\n",
            "Epoch 56/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.5567 - accuracy: 0.7442 - val_loss: 1.6113 - val_accuracy: 0.7344\n",
            "Epoch 57/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.5923 - accuracy: 0.7456 - val_loss: 1.4857 - val_accuracy: 0.7578\n",
            "Epoch 58/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.4775 - accuracy: 0.7870 - val_loss: 1.6912 - val_accuracy: 0.6797\n",
            "Epoch 59/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.4525 - accuracy: 0.7741 - val_loss: 1.5078 - val_accuracy: 0.7422\n",
            "Epoch 60/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.4327 - accuracy: 0.7815 - val_loss: 1.4280 - val_accuracy: 0.7656\n",
            "Epoch 61/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.4130 - accuracy: 0.7788 - val_loss: 1.4422 - val_accuracy: 0.7656\n",
            "Epoch 62/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.3755 - accuracy: 0.7897 - val_loss: 1.3997 - val_accuracy: 0.7422\n",
            "Epoch 63/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.3786 - accuracy: 0.7843 - val_loss: 1.4936 - val_accuracy: 0.6719\n",
            "Epoch 64/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.3338 - accuracy: 0.7897 - val_loss: 1.3638 - val_accuracy: 0.7344\n",
            "Epoch 65/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.3017 - accuracy: 0.8039 - val_loss: 1.4394 - val_accuracy: 0.6953\n",
            "Epoch 66/75\n",
            "12/12 [==============================] - 17s 1s/step - loss: 1.3638 - accuracy: 0.7884 - val_loss: 1.4736 - val_accuracy: 0.7031\n",
            "Epoch 67/75\n",
            "12/12 [==============================] - 17s 1s/step - loss: 1.3546 - accuracy: 0.7883 - val_loss: 1.4226 - val_accuracy: 0.7266\n",
            "Epoch 68/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.2535 - accuracy: 0.8223 - val_loss: 1.2881 - val_accuracy: 0.7812\n",
            "Epoch 69/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.2775 - accuracy: 0.7992 - val_loss: 1.3597 - val_accuracy: 0.7109\n",
            "Epoch 70/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.1791 - accuracy: 0.8121 - val_loss: 1.3842 - val_accuracy: 0.7500\n",
            "Epoch 71/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.1946 - accuracy: 0.8216 - val_loss: 1.3733 - val_accuracy: 0.7031\n",
            "Epoch 72/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.2392 - accuracy: 0.7965 - val_loss: 1.3110 - val_accuracy: 0.7422\n",
            "Epoch 73/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.1841 - accuracy: 0.8202 - val_loss: 1.3089 - val_accuracy: 0.7188\n",
            "Epoch 74/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.2339 - accuracy: 0.8100 - val_loss: 1.4180 - val_accuracy: 0.7344\n",
            "Epoch 75/75\n",
            "12/12 [==============================] - 16s 1s/step - loss: 1.2279 - accuracy: 0.8155 - val_loss: 1.5003 - val_accuracy: 0.7031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f32003434e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK_SdUHUdpyD",
        "outputId": "47e32f04-1453-47ad-af7d-e755bf61f855"
      },
      "source": [
        "# save the model to file\n",
        "print(\"[INFO] serializing model...\")\n",
        "model.save(MODEL_PATH, overwrite=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] serializing model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/dataset/catsanddogs/train/output/alexnet_dogs_vs_cats.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1tVZ2g-dpyD"
      },
      "source": [
        "# close the HDF5 datasets\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}