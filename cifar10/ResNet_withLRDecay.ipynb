{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "mount_file_id": "1_ueqYaHmMpeZWZAnlA1ABR8SXPu09USW",
      "authorship_tag": "ABX9TyMnryf8bP8gbFVrU8ry7PoA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sripraks/CNN/blob/main/cifar10/ResNet_withLRDecay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1nnpXyPtivv"
      },
      "source": [
        "import matplotlib\r\n",
        "matplotlib.use(\"Agg\")\r\n",
        "\r\n",
        "# import the necessary packages\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.optimizers import SGD\r\n",
        "from tensorflow.keras.datasets import cifar10\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "import numpy as np  \r\n",
        "import argparse\r\n",
        "import sys\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpxnK4bwtzKn"
      },
      "source": [
        "# import the necessary packages\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.layers import Conv2D\r\n",
        "from tensorflow.keras.layers import AveragePooling2D\r\n",
        "from tensorflow.keras.layers import MaxPooling2D\r\n",
        "from tensorflow.keras.layers import ZeroPadding2D\r\n",
        "from tensorflow.keras.layers import Activation\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import Flatten\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import add\r\n",
        "from tensorflow.keras.regularizers import l2\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "\r\n",
        "class ResNet:\r\n",
        "\t@staticmethod\r\n",
        "\tdef residual_module(data, K, stride, chanDim, red=False,\r\n",
        "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\r\n",
        "\t\t# the shortcut branch of the ResNet module should be\r\n",
        "\t\t# initialize as the input (identity) data\r\n",
        "\t\tshortcut = data\r\n",
        "\r\n",
        "\t\t# the first block of the ResNet module are the 1x1 CONVs\r\n",
        "\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\tmomentum=bnMom)(data)\r\n",
        "\t\tact1 = Activation(\"relu\")(bn1)\r\n",
        "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\r\n",
        "\t\t\tkernel_regularizer=l2(reg))(act1)\r\n",
        "\r\n",
        "\t\t# the second block of the ResNet module are the 3x3 CONVs\r\n",
        "\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\tmomentum=bnMom)(conv1)\r\n",
        "\t\tact2 = Activation(\"relu\")(bn2)\r\n",
        "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\r\n",
        "\t\t\tpadding=\"same\", use_bias=False,\r\n",
        "\t\t\tkernel_regularizer=l2(reg))(act2)\r\n",
        "\r\n",
        "\t\t# the third block of the ResNet module is another set of 1x1\r\n",
        "\t\t# CONVs\r\n",
        "\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\tmomentum=bnMom)(conv2)\r\n",
        "\t\tact3 = Activation(\"relu\")(bn3)\r\n",
        "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\r\n",
        "\t\t\tkernel_regularizer=l2(reg))(act3)\r\n",
        "\r\n",
        "\t\t# if we are to reduce the spatial size, apply a CONV layer to\r\n",
        "\t\t# the shortcut\r\n",
        "\t\tif red:\r\n",
        "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\r\n",
        "\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\r\n",
        "\r\n",
        "\t\t# add together the shortcut and the final CONV\r\n",
        "\t\tx = add([conv3, shortcut])\r\n",
        "\r\n",
        "\t\t# return the addition as the output of the ResNet module\r\n",
        "\t\treturn x\r\n",
        "\r\n",
        "\t@staticmethod\r\n",
        "\tdef build(width, height, depth, classes, stages, filters,\r\n",
        "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\r\n",
        "\t\t# initialize the input shape to be \"channels last\" and the\r\n",
        "\t\t# channels dimension itself\r\n",
        "\t\tinputShape = (height, width, depth)\r\n",
        "\t\tchanDim = -1\r\n",
        "\r\n",
        "\t\t# if we are using \"channels first\", update the input shape\r\n",
        "\t\t# and channels dimension\r\n",
        "\t\tif K.image_data_format() == \"channels_first\":\r\n",
        "\t\t\tinputShape = (depth, height, width)\r\n",
        "\t\t\tchanDim = 1\r\n",
        "\r\n",
        "\t\t# set the input and apply BN\r\n",
        "\t\tinputs = Input(shape=inputShape)\r\n",
        "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\tmomentum=bnMom)(inputs)\r\n",
        "\r\n",
        "\t\t# check if we are utilizing the CIFAR dataset\r\n",
        "\t\tif dataset == \"cifar\":\r\n",
        "\t\t\t# apply a single CONV layer\r\n",
        "\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\r\n",
        "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\r\n",
        "\r\n",
        "\t\t# check to see if we are using the Tiny ImageNet dataset\r\n",
        "\t\telif dataset == \"tiny_imagenet\":\r\n",
        "\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\r\n",
        "\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\r\n",
        "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\r\n",
        "\t\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\t\tmomentum=bnMom)(x)\r\n",
        "\t\t\tx = Activation(\"relu\")(x)\r\n",
        "\t\t\tx = ZeroPadding2D((1, 1))(x)\r\n",
        "\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\r\n",
        "\r\n",
        "\t\t# loop over the number of stages\r\n",
        "\t\tfor i in range(0, len(stages)):\r\n",
        "\t\t\t# initialize the stride, then apply a residual module\r\n",
        "\t\t\t# used to reduce the spatial size of the input volume\r\n",
        "\t\t\tstride = (1, 1) if i == 0 else (2, 2)\r\n",
        "\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\r\n",
        "\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\r\n",
        "\r\n",
        "\t\t\t# loop over the number of layers in the stage\r\n",
        "\t\t\tfor j in range(0, stages[i] - 1):\r\n",
        "\t\t\t\t# apply a ResNet module\r\n",
        "\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\r\n",
        "\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\r\n",
        "\r\n",
        "\t\t# apply BN => ACT => POOL\r\n",
        "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\r\n",
        "\t\t\tmomentum=bnMom)(x)\r\n",
        "\t\tx = Activation(\"relu\")(x)\r\n",
        "\t\tx = AveragePooling2D((8, 8))(x)\r\n",
        "\r\n",
        "\t\t# softmax classifier\r\n",
        "\t\tx = Flatten()(x)\r\n",
        "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\r\n",
        "\t\tx = Activation(\"softmax\")(x)\r\n",
        "\r\n",
        "\t\t# create the model\r\n",
        "\t\tmodel = Model(inputs, x, name=\"resnet\")\r\n",
        "\r\n",
        "\t\t# return the constructed network architecture\r\n",
        "\t\treturn model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwveo7bCt7OQ"
      },
      "source": [
        "# import the necessary packages\r\n",
        "from tensorflow.keras.callbacks import Callback\r\n",
        "import os\r\n",
        "\r\n",
        "class EpochCheckpoint(Callback):\r\n",
        "\tdef __init__(self, outputPath, every=5, startAt=0):\r\n",
        "\t\t# call the parent constructor\r\n",
        "\t\tsuper(Callback, self).__init__()\r\n",
        "\r\n",
        "\t\t# store the base output path for the model, the number of\r\n",
        "\t\t# epochs that must pass before the model is serialized to\r\n",
        "\t\t# disk and the current epoch value\r\n",
        "\t\tself.outputPath = outputPath\r\n",
        "\t\tself.every = every\r\n",
        "\t\tself.intEpoch = startAt\r\n",
        "\r\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\r\n",
        "\t\t# check to see if the model should be serialized to disk\r\n",
        "\t\tif (self.intEpoch + 1) % self.every == 0:\r\n",
        "\t\t\tp = os.path.sep.join([self.outputPath,\r\n",
        "\t\t\t\t\"epoch_{}.hdf5\".format(self.intEpoch + 1)])\r\n",
        "\t\t\tself.model.save(p, overwrite=True)\r\n",
        "\r\n",
        "\t\t# increment the internal epoch counter\r\n",
        "\t\tself.intEpoch += 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyocy9L9t-Ck"
      },
      "source": [
        "# import the necessary packages\r\n",
        "from tensorflow.keras.callbacks import BaseLogger\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "import os\r\n",
        "\r\n",
        "class TrainingMonitor(BaseLogger):\r\n",
        "\tdef __init__(self, figPath, jsonPath=None, startAt=0):\r\n",
        "\t\t# store the output path for the figure, the path to the JSON\r\n",
        "\t\t# serialized file, and the starting epoch\r\n",
        "\t\tsuper(TrainingMonitor, self).__init__()\r\n",
        "\t\tself.figPath = figPath\r\n",
        "\t\tself.jsonPath = jsonPath\r\n",
        "\t\tself.startAt = startAt\r\n",
        "\r\n",
        "\tdef on_train_begin(self, logs={}):\r\n",
        "\t\t# initialize the history dictionary\r\n",
        "\t\tself.H = {}\r\n",
        "\r\n",
        "\t\t# if the JSON history path exists, load the training history\r\n",
        "\t\tif self.jsonPath is not None:\r\n",
        "\t\t\tif os.path.exists(self.jsonPath):\r\n",
        "\t\t\t\tself.H = json.loads(open(self.jsonPath).read())\r\n",
        "\r\n",
        "\t\t\t\t# check to see if a starting epoch was supplied\r\n",
        "\t\t\t\tif self.startAt > 0:\r\n",
        "\t\t\t\t\t# loop over the entries in the history log and\r\n",
        "\t\t\t\t\t# trim any entries that are past the starting\r\n",
        "\t\t\t\t\t# epoch\r\n",
        "\t\t\t\t\tfor k in self.H.keys():\r\n",
        "\t\t\t\t\t\tself.H[k] = self.H[k][:self.startAt]\r\n",
        "\r\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\r\n",
        "\t\t# loop over the logs and update the loss, accuracy, etc.\r\n",
        "\t\t# for the entire training process\r\n",
        "\t\tfor (k, v) in logs.items():\r\n",
        "\t\t\tl = self.H.get(k, [])\r\n",
        "\t\t\tl.append(float(v))\r\n",
        "\t\t\tself.H[k] = l\r\n",
        "\r\n",
        "\t\t# check to see if the training history should be serialized\r\n",
        "\t\t# to file\r\n",
        "\t\tif self.jsonPath is not None:\r\n",
        "\t\t\tf = open(self.jsonPath, \"w\")\r\n",
        "\t\t\tf.write(json.dumps(self.H))\r\n",
        "\t\t\tf.close()\r\n",
        "\r\n",
        "\t\t# ensure at least two epochs have passed before plotting\r\n",
        "\t\t# (epoch starts at zero)\r\n",
        "\t\tif len(self.H[\"loss\"]) > 1:\r\n",
        "\t\t\t# plot the training loss and accuracy\r\n",
        "\t\t\tN = np.arange(0, len(self.H[\"loss\"]))\r\n",
        "\t\t\tplt.style.use(\"ggplot\")\r\n",
        "\t\t\tplt.figure()\r\n",
        "\t\t\tplt.plot(N, self.H[\"loss\"], label=\"train_loss\")\r\n",
        "\t\t\tplt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\r\n",
        "\t\t\tplt.plot(N, self.H[\"accuracy\"], label=\"train_acc\")\r\n",
        "\t\t\tplt.plot(N, self.H[\"val_accuracy\"], label=\"val_acc\")\r\n",
        "\t\t\tplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(\r\n",
        "\t\t\t\tlen(self.H[\"loss\"])))\r\n",
        "\t\t\tplt.xlabel(\"Epoch #\")\r\n",
        "\t\t\tplt.ylabel(\"Loss/Accuracy\")\r\n",
        "\t\t\tplt.legend()\r\n",
        "\r\n",
        "\t\t\t# save the figure\r\n",
        "\t\t\tplt.savefig(self.figPath)\r\n",
        "\t\t\tplt.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWc4tMN-uPcP"
      },
      "source": [
        "# set a high recursion limit so Theano doesn't complain\r\n",
        "sys.setrecursionlimit(5000)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj5xgisqvKwK"
      },
      "source": [
        "# define the total number of epochs to train for along with the\r\n",
        "# initial learning rate\r\n",
        "NUM_EPOCHS = 100\r\n",
        "INIT_LR = 1e-1\r\n",
        "\r\n",
        "def poly_decay(epoch):\r\n",
        "\t# initialize the maximum number of epochs, base learning rate,\r\n",
        "\t# and power of the polynomial\r\n",
        "\tmaxEpochs = NUM_EPOCHS\r\n",
        "\tbaseLR = INIT_LR\r\n",
        "\tpower = 1.0\r\n",
        "\r\n",
        "\t# compute the new learning rate based on polynomial decay\r\n",
        "\talpha = baseLR * (1 - (epoch / float(maxEpochs))) ** power\r\n",
        "\r\n",
        "\t# return the new learning rate\r\n",
        "\treturn alpha"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCMNOeuuuTOF",
        "outputId": "e745ee2b-7bbb-42a6-f0be-74d393cbec0f"
      },
      "source": [
        "# load the training and testing data, converting the images from\r\n",
        "# integers to floats\r\n",
        "print(\"[INFO] loading CIFAR-10 data...\")\r\n",
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()\r\n",
        "trainX = trainX.astype(\"float\")\r\n",
        "testX = testX.astype(\"float\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading CIFAR-10 data...\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGSLYT7guUkg"
      },
      "source": [
        "\r\n",
        "# apply mean subtraction to the data\r\n",
        "mean = np.mean(trainX, axis=0)\r\n",
        "trainX -= mean\r\n",
        "testX -= mean"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k1OjrlruWvF"
      },
      "source": [
        "# convert the labels from integers to vectors\r\n",
        "lb = LabelBinarizer()\r\n",
        "trainY = lb.fit_transform(trainY)\r\n",
        "testY = lb.transform(testY)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8prD0svUuYPy"
      },
      "source": [
        "# construct the image generator for data augmentation\r\n",
        "aug = ImageDataGenerator(width_shift_range=0.1,height_shift_range=0.1, horizontal_flip=True,fill_mode=\"nearest\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URrrAm9nvZXI"
      },
      "source": [
        "# construct the set of callbacks\r\n",
        "figPath = os.path.sep.join([\"/content/drive/MyDrive/dataset/flowers17/output\", \"{}.png\".format(os.getpid())])\r\n",
        "jsonPath = os.path.sep.join([\"/content/drive/MyDrive/dataset/flowers17/output\", \"{}.json\".format(os.getpid())])\r\n",
        "callbacks = [TrainingMonitor(figPath, jsonPath=jsonPath),LearningRateScheduler(poly_decay)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK2sSJmMua7z",
        "outputId": "415c6caa-d891-4ea0-98d5-ff8c008c0065"
      },
      "source": [
        "# if there is no specific model checkpoint supplied, then initialize\r\n",
        "# the network (ResNet-56) and compile the model\r\n",
        "#if args[\"model\"] is None:\r\n",
        "print(\"[INFO] compiling model...\")\r\n",
        "opt = SGD(lr=INIT_LR)\r\n",
        "model = ResNet.build(32, 32, 3, 10, (9, 9, 9),(64, 64, 128, 256), reg=0.0005)\r\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KMJbo8wueVv",
        "outputId": "86584f1d-980a-4812-b04b-9df4b249695c"
      },
      "source": [
        "# train the network\r\n",
        "print(\"[INFO] training network...\")\r\n",
        "model.fit_generator(\r\n",
        "\taug.flow(trainX, trainY, batch_size=128),\r\n",
        "\tvalidation_data=(testX, testY),\r\n",
        "\tsteps_per_epoch=len(trainX) // 128, epochs=NUM_EPOCHS,\r\n",
        "\tcallbacks=callbacks, verbose=1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "390/390 [==============================] - 76s 162ms/step - loss: 2.3984 - accuracy: 0.2704 - val_loss: 1.9927 - val_accuracy: 0.4280\n",
            "Epoch 2/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 1.9010 - accuracy: 0.4742 - val_loss: 1.7053 - val_accuracy: 0.5437\n",
            "Epoch 3/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.6884 - accuracy: 0.5556 - val_loss: 1.5925 - val_accuracy: 0.5975\n",
            "Epoch 4/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 1.5344 - accuracy: 0.6150 - val_loss: 1.5733 - val_accuracy: 0.6094\n",
            "Epoch 5/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 1.4085 - accuracy: 0.6629 - val_loss: 1.4255 - val_accuracy: 0.6590\n",
            "Epoch 6/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 1.3154 - accuracy: 0.6967 - val_loss: 1.3156 - val_accuracy: 0.6962\n",
            "Epoch 7/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.2357 - accuracy: 0.7221 - val_loss: 1.2764 - val_accuracy: 0.7121\n",
            "Epoch 8/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.1516 - accuracy: 0.7529 - val_loss: 1.2426 - val_accuracy: 0.7222\n",
            "Epoch 9/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 1.1113 - accuracy: 0.7641 - val_loss: 1.2574 - val_accuracy: 0.7211\n",
            "Epoch 10/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.0505 - accuracy: 0.7860 - val_loss: 1.1428 - val_accuracy: 0.7583\n",
            "Epoch 11/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.0149 - accuracy: 0.7974 - val_loss: 1.1256 - val_accuracy: 0.7583\n",
            "Epoch 12/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.9623 - accuracy: 0.8162 - val_loss: 1.1002 - val_accuracy: 0.7768\n",
            "Epoch 13/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.9440 - accuracy: 0.8193 - val_loss: 1.0746 - val_accuracy: 0.7809\n",
            "Epoch 14/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.9213 - accuracy: 0.8255 - val_loss: 1.0298 - val_accuracy: 0.7939\n",
            "Epoch 15/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.8839 - accuracy: 0.8395 - val_loss: 1.0049 - val_accuracy: 0.8005\n",
            "Epoch 16/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.8615 - accuracy: 0.8483 - val_loss: 0.9851 - val_accuracy: 0.8089\n",
            "Epoch 17/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.8334 - accuracy: 0.8572 - val_loss: 0.9425 - val_accuracy: 0.8183\n",
            "Epoch 18/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.8190 - accuracy: 0.8601 - val_loss: 0.9278 - val_accuracy: 0.8235\n",
            "Epoch 19/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.8026 - accuracy: 0.8614 - val_loss: 0.9364 - val_accuracy: 0.8221\n",
            "Epoch 20/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.7832 - accuracy: 0.8703 - val_loss: 0.9877 - val_accuracy: 0.8056\n",
            "Epoch 21/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.7597 - accuracy: 0.8752 - val_loss: 0.9131 - val_accuracy: 0.8311\n",
            "Epoch 22/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.7483 - accuracy: 0.8760 - val_loss: 0.8747 - val_accuracy: 0.8374\n",
            "Epoch 23/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.7287 - accuracy: 0.8823 - val_loss: 1.0475 - val_accuracy: 0.7946\n",
            "Epoch 24/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.7142 - accuracy: 0.8877 - val_loss: 0.8912 - val_accuracy: 0.8334\n",
            "Epoch 25/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.7035 - accuracy: 0.8915 - val_loss: 0.9661 - val_accuracy: 0.8161\n",
            "Epoch 26/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.6906 - accuracy: 0.8934 - val_loss: 0.8579 - val_accuracy: 0.8431\n",
            "Epoch 27/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.6747 - accuracy: 0.8984 - val_loss: 0.8694 - val_accuracy: 0.8400\n",
            "Epoch 28/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.6603 - accuracy: 0.9013 - val_loss: 0.8404 - val_accuracy: 0.8470\n",
            "Epoch 29/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.6501 - accuracy: 0.9045 - val_loss: 0.8351 - val_accuracy: 0.8478\n",
            "Epoch 30/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.6475 - accuracy: 0.9066 - val_loss: 0.8199 - val_accuracy: 0.8528\n",
            "Epoch 31/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.6289 - accuracy: 0.9103 - val_loss: 0.8721 - val_accuracy: 0.8376\n",
            "Epoch 32/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.6198 - accuracy: 0.9139 - val_loss: 0.9156 - val_accuracy: 0.8292\n",
            "Epoch 33/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.6039 - accuracy: 0.9169 - val_loss: 0.7860 - val_accuracy: 0.8590\n",
            "Epoch 34/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5985 - accuracy: 0.9176 - val_loss: 0.7954 - val_accuracy: 0.8555\n",
            "Epoch 35/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5923 - accuracy: 0.9195 - val_loss: 0.8680 - val_accuracy: 0.8422\n",
            "Epoch 36/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5850 - accuracy: 0.9212 - val_loss: 0.7753 - val_accuracy: 0.8610\n",
            "Epoch 37/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5709 - accuracy: 0.9243 - val_loss: 0.7608 - val_accuracy: 0.8673\n",
            "Epoch 38/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5638 - accuracy: 0.9276 - val_loss: 0.7978 - val_accuracy: 0.8575\n",
            "Epoch 39/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5534 - accuracy: 0.9307 - val_loss: 0.8014 - val_accuracy: 0.8566\n",
            "Epoch 40/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5447 - accuracy: 0.9333 - val_loss: 0.8418 - val_accuracy: 0.8466\n",
            "Epoch 41/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5402 - accuracy: 0.9333 - val_loss: 0.8659 - val_accuracy: 0.8409\n",
            "Epoch 42/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5348 - accuracy: 0.9328 - val_loss: 0.8523 - val_accuracy: 0.8490\n",
            "Epoch 43/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5261 - accuracy: 0.9356 - val_loss: 0.8183 - val_accuracy: 0.8558\n",
            "Epoch 44/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5156 - accuracy: 0.9385 - val_loss: 0.8141 - val_accuracy: 0.8569\n",
            "Epoch 45/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5050 - accuracy: 0.9441 - val_loss: 0.7416 - val_accuracy: 0.8756\n",
            "Epoch 46/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.5061 - accuracy: 0.9416 - val_loss: 0.8189 - val_accuracy: 0.8568\n",
            "Epoch 47/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4969 - accuracy: 0.9434 - val_loss: 0.7477 - val_accuracy: 0.8695\n",
            "Epoch 48/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4855 - accuracy: 0.9475 - val_loss: 0.7483 - val_accuracy: 0.8679\n",
            "Epoch 49/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4864 - accuracy: 0.9458 - val_loss: 0.7922 - val_accuracy: 0.8639\n",
            "Epoch 50/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4793 - accuracy: 0.9471 - val_loss: 0.7563 - val_accuracy: 0.8714\n",
            "Epoch 51/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4687 - accuracy: 0.9533 - val_loss: 0.7344 - val_accuracy: 0.8749\n",
            "Epoch 52/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4680 - accuracy: 0.9513 - val_loss: 0.7726 - val_accuracy: 0.8619\n",
            "Epoch 53/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4594 - accuracy: 0.9515 - val_loss: 0.7340 - val_accuracy: 0.8732\n",
            "Epoch 54/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.4493 - accuracy: 0.9555 - val_loss: 0.7230 - val_accuracy: 0.8765\n",
            "Epoch 55/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.4522 - accuracy: 0.9540 - val_loss: 0.7291 - val_accuracy: 0.8751\n",
            "Epoch 56/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4461 - accuracy: 0.9572 - val_loss: 0.7298 - val_accuracy: 0.8793\n",
            "Epoch 57/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4331 - accuracy: 0.9597 - val_loss: 0.7822 - val_accuracy: 0.8674\n",
            "Epoch 58/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4378 - accuracy: 0.9586 - val_loss: 0.7448 - val_accuracy: 0.8719\n",
            "Epoch 59/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.4274 - accuracy: 0.9622 - val_loss: 0.7698 - val_accuracy: 0.8678\n",
            "Epoch 60/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4286 - accuracy: 0.9596 - val_loss: 0.7427 - val_accuracy: 0.8763\n",
            "Epoch 61/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.4146 - accuracy: 0.9645 - val_loss: 0.6951 - val_accuracy: 0.8859\n",
            "Epoch 62/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4147 - accuracy: 0.9662 - val_loss: 0.7882 - val_accuracy: 0.8705\n",
            "Epoch 63/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4031 - accuracy: 0.9682 - val_loss: 0.7473 - val_accuracy: 0.8760\n",
            "Epoch 64/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.4038 - accuracy: 0.9658 - val_loss: 0.7081 - val_accuracy: 0.8856\n",
            "Epoch 65/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.4030 - accuracy: 0.9664 - val_loss: 0.7169 - val_accuracy: 0.8817\n",
            "Epoch 66/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3992 - accuracy: 0.9670 - val_loss: 0.7731 - val_accuracy: 0.8709\n",
            "Epoch 67/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3877 - accuracy: 0.9703 - val_loss: 0.7430 - val_accuracy: 0.8750\n",
            "Epoch 68/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3896 - accuracy: 0.9709 - val_loss: 0.6813 - val_accuracy: 0.8896\n",
            "Epoch 69/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3838 - accuracy: 0.9728 - val_loss: 0.7137 - val_accuracy: 0.8871\n",
            "Epoch 70/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3777 - accuracy: 0.9739 - val_loss: 0.7264 - val_accuracy: 0.8801\n",
            "Epoch 71/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3712 - accuracy: 0.9762 - val_loss: 0.7180 - val_accuracy: 0.8843\n",
            "Epoch 72/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3689 - accuracy: 0.9762 - val_loss: 0.7531 - val_accuracy: 0.8792\n",
            "Epoch 73/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3603 - accuracy: 0.9789 - val_loss: 0.7161 - val_accuracy: 0.8849\n",
            "Epoch 74/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3630 - accuracy: 0.9787 - val_loss: 0.7127 - val_accuracy: 0.8844\n",
            "Epoch 75/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3577 - accuracy: 0.9791 - val_loss: 0.7139 - val_accuracy: 0.8859\n",
            "Epoch 76/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3514 - accuracy: 0.9806 - val_loss: 0.7271 - val_accuracy: 0.8881\n",
            "Epoch 77/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3488 - accuracy: 0.9824 - val_loss: 0.7642 - val_accuracy: 0.8822\n",
            "Epoch 78/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3476 - accuracy: 0.9820 - val_loss: 0.7290 - val_accuracy: 0.8814\n",
            "Epoch 79/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3443 - accuracy: 0.9824 - val_loss: 0.7322 - val_accuracy: 0.8859\n",
            "Epoch 80/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3462 - accuracy: 0.9815 - val_loss: 0.7089 - val_accuracy: 0.8873\n",
            "Epoch 81/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3433 - accuracy: 0.9824 - val_loss: 0.7110 - val_accuracy: 0.8885\n",
            "Epoch 82/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3350 - accuracy: 0.9851 - val_loss: 0.7027 - val_accuracy: 0.8931\n",
            "Epoch 83/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3303 - accuracy: 0.9860 - val_loss: 0.6847 - val_accuracy: 0.8947\n",
            "Epoch 84/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3312 - accuracy: 0.9861 - val_loss: 0.6813 - val_accuracy: 0.8949\n",
            "Epoch 85/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3282 - accuracy: 0.9872 - val_loss: 0.6977 - val_accuracy: 0.8925\n",
            "Epoch 86/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3266 - accuracy: 0.9871 - val_loss: 0.6995 - val_accuracy: 0.8928\n",
            "Epoch 87/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3239 - accuracy: 0.9876 - val_loss: 0.6986 - val_accuracy: 0.8927\n",
            "Epoch 88/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3204 - accuracy: 0.9892 - val_loss: 0.6922 - val_accuracy: 0.8968\n",
            "Epoch 89/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3202 - accuracy: 0.9884 - val_loss: 0.6921 - val_accuracy: 0.8943\n",
            "Epoch 90/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3158 - accuracy: 0.9903 - val_loss: 0.6772 - val_accuracy: 0.8978\n",
            "Epoch 91/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3162 - accuracy: 0.9901 - val_loss: 0.6811 - val_accuracy: 0.8961\n",
            "Epoch 92/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3151 - accuracy: 0.9904 - val_loss: 0.6799 - val_accuracy: 0.8956\n",
            "Epoch 93/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3124 - accuracy: 0.9916 - val_loss: 0.6844 - val_accuracy: 0.8978\n",
            "Epoch 94/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3125 - accuracy: 0.9907 - val_loss: 0.6831 - val_accuracy: 0.8969\n",
            "Epoch 95/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3100 - accuracy: 0.9915 - val_loss: 0.6793 - val_accuracy: 0.8983\n",
            "Epoch 96/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3058 - accuracy: 0.9933 - val_loss: 0.6839 - val_accuracy: 0.8962\n",
            "Epoch 97/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3086 - accuracy: 0.9924 - val_loss: 0.6767 - val_accuracy: 0.8983\n",
            "Epoch 98/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3049 - accuracy: 0.9939 - val_loss: 0.6769 - val_accuracy: 0.8990\n",
            "Epoch 99/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3057 - accuracy: 0.9930 - val_loss: 0.6767 - val_accuracy: 0.8974\n",
            "Epoch 100/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.3044 - accuracy: 0.9934 - val_loss: 0.6780 - val_accuracy: 0.8986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb71d10ba20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eNXXYyAwG1J",
        "outputId": "ed42d7ef-6f19-4dab-dfd4-748cac2369ab"
      },
      "source": [
        "# save the network to disk\r\n",
        "print(\"[INFO] serializing network...\")\r\n",
        "model.save(\"/content/drive/MyDrive/dataset/flowers17/output/ResnetwithDecay.hdf5\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] serializing network...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}